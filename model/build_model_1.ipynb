{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# update batch size\n",
    "batch_size = 20\n",
    "\n",
    "# number of batch in training\n",
    "num_of_batch = 10\n",
    "\n",
    "# fixed number of time steps in one episode (not used)\n",
    "trading_period = 60\n",
    "\n",
    "# 1 is zscore\n",
    "num_features = 1\n",
    "\n",
    "# 0 is no position. 1 is long the spread. 2 is short the spread.\n",
    "a_num = position_num = 3\n",
    "\n",
    "# RNN hidden state dimension\n",
    "h_dim = 70\n",
    "\n",
    "# number of RNN layer\n",
    "num_layers = 1\n",
    "\n",
    "# number of layer1 output\n",
    "layer1_out_num = 100\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "# discount factor in reinforcement learning\n",
    "gamma = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_random_history(history):\n",
    "    \"\"\"Sample some pairs and get the history of those pairs. The history should have\n",
    "    three dimension. The first dimension is for time. The second dimension is indexed\n",
    "    by features name. The third dimension is the index of training instance.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_input_history(history):\n",
    "    \"\"\"Slicing history in its second dimension.\"\"\"\n",
    "    pass\n",
    "\n",
    "def sample_action(logits):\n",
    "    dist = tf.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    # 1-D Tensor where the i-th element correspond to a sample from\n",
    "    # the i-th categorical distribution\n",
    "    return dist.sample()\n",
    "\n",
    "def discount_rewards(r, all_actions):\n",
    "    \"\"\"\n",
    "    r is a numpy array in the shape of (n, batch_size).\n",
    "    all_actions is a numpy array in the same shape as r.\n",
    "    \n",
    "    return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "    result = np.zeros_like(r, dtype=float)\n",
    "    n = r.shape[0]\n",
    "    sum_ = np.zeros_like(r[0], dtype=float)\n",
    "    pre_action = all_actions[n-1]\n",
    "    for i in range(n-1,-1,-1):\n",
    "        sum_ *= gamma\n",
    "        \n",
    "        # when the previous action(position) not equal to the current one,\n",
    "        # set the previous sum of reward to be zero.\n",
    "        sum_ = sum_*(all_actions[i]==pre_action) + r[i]\n",
    "        result[i] = sum_\n",
    "        \n",
    "        # update pre_action\n",
    "        pre_action = all_actions[i]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def loss(all_logits, all_actions, all_advantages):\n",
    "    neg_log_select_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_logits, labels=all_actions)\n",
    "    \n",
    "    # 0 axis is the time axis. 1 axis is the batch axis\n",
    "    return tf.reduce_mean(neg_log_select_prob * all_advantages, 0)\n",
    "\n",
    "# classes\n",
    "class TradingPolicyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TradingPolicyModel, self).__init__()\n",
    "        self.dense1 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.logits = tf.layers.Dense(units=a_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        x = self.dense1(inputs)\n",
    "        logits = self.logits(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class StateEncodingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(StateEncodingModel, self).__init__()\n",
    "        self.cell_layer = tf.contrib.rnn.LSTMCell(h_dim)\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell([self.cell_layer] * num_layers)\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        oberservation, self.state = self.cell(inputs, self.state)\n",
    "        return oberservation\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "class TradingEnvironment():\n",
    "    \"\"\"Trading environment for reinforcement learning training.\n",
    "    \n",
    "    Arguments:\n",
    "        state_encoding_model: the model that encode past input_history data into a state\n",
    "        vector which will be fed as input to the policy network.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_encoding_model):\n",
    "        # do some initialization\n",
    "        self.state_encoding_model = state_encoding_model\n",
    "        self._reset_env()\n",
    "        \n",
    "    def _reset_env(self):\n",
    "        self.t = 0\n",
    "        self.state_encoding_model.reset_state()\n",
    "\n",
    "        # 0 is no position. 1 is long the spread. 2 is short the spread\n",
    "        self.position = np.zeros(batch_size)\n",
    "\n",
    "        # prepare a batch of history and input_history\n",
    "        self.history = get_random_history(batch_size)\n",
    "        self.input_history = compute_input_history(self.history)\n",
    "        \n",
    "        # create or update self.state variable\n",
    "        self.update_state()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return an initial state for the trading environment\"\"\"\n",
    "        if self.t == 0:\n",
    "            return self.state\n",
    "        else:\n",
    "            self._reset_env()\n",
    "            return self.state\n",
    "    \n",
    "    def compute_reward(self, action):\n",
    "        # if action is 0, no reward.\n",
    "        # if action is 1 or 2, can compute immediate return as immediate reward\n",
    "        r = np.zeros_like(action, dtype=float)        \n",
    "        cur_his = self.history[self.t]\n",
    "        nex_his = self.history[self.t+1]\n",
    "        \n",
    "        # compute for each training instance in a batch\n",
    "        for i, a in enumerate(action):\n",
    "            y_p = cur_his[\"y_close\"][i]\n",
    "            x_p = cur_his[\"x_close\"][i]\n",
    "            nex_y_p = nex_his[\"y_close\"][i]\n",
    "            nex_x_p = nex_his[\"x_close\"][i]\n",
    "            if a == 1:\n",
    "                r[i] = math.log(nex_y_p/y_p) + math.log(x_p/nex_x_p)\n",
    "            elif a == 2:\n",
    "                r[i] = math.log(nex_x_p/x_p) + math.log(y_p/nex_y_p)\n",
    "        return r\n",
    "                \n",
    "    \n",
    "    def update_state(self):\n",
    "        # concate next_input_history and next position to form next partial state\n",
    "        partial_state = tf.concat([self.input_history[self.t], tf.one_hot(self.position, position_num)], 1)\n",
    "        \n",
    "        # update state\n",
    "        self.state = self.state_encoding_model(partial_state)        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Given the current state and action, return the reward, next state and done.\n",
    "        This function should be called after reset.\n",
    "        \n",
    "        reward is of type numpy array. state is of type tensor. done is of type boolean.\n",
    "        \n",
    "        \n",
    "        Arguments:\n",
    "            action: a numpy array containing the current action for each training pair.\n",
    "\n",
    "        Note that we follow the convention where the trajectory is indexed as s_0, a_0, r_0,\n",
    "        s_1, ... . Therefore t is updated just after computing the reward is computed and\n",
    "        before computing next state.\n",
    "        \"\"\"\n",
    "        # r_t\n",
    "        r = self.compute_reward(action)\n",
    "\n",
    "        # t = t+1\n",
    "        self.t += 1\n",
    "        \n",
    "        # compute s_(t+1)\n",
    "        self.position = action\n",
    "        self.update_state()\n",
    "\n",
    "        return r, self.state, self.t == batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:<tensorflow.python.ops.rnn_cell_impl.LSTMCell object at 0x7f17d5847898>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2a462c781280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTradingPolicyModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstate_encoding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStateEncodingModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTradingEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_encoding_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mall_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_encode_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d3bd1388c0a4>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_encoding_model)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# do some initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_encoding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_encoding_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d3bd1388c0a4>\u001b[0m in \u001b[0;36m_reset_env\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;31m# create or update self.state variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d3bd1388c0a4>\u001b[0m in \u001b[0;36mupdate_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# concate next_input_history and next position to form next partial state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mpartial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# update state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "pi = TradingPolicyModel()\n",
    "state_encoding_model = StateEncodingModel()\n",
    "env = TradingEnvironment(state_encoding_model)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "all_variables = state_encode_model.variables + pi.variables\n",
    "\n",
    "for batch in range(num_of_batch):\n",
    "    # for training reference only\n",
    "    average_total_r = np.zeros(batch_size)\n",
    "    \n",
    "    # saving for update\n",
    "    all_logits = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    with tf.GradientTape() as gt:\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "\n",
    "        # internally the episode length is fixed by trading_period\n",
    "        while not done:\n",
    "            logits = pi(0)\n",
    "            a = sample_action(logits)\n",
    "            r, next_s, done = senv.step(a.numpy())\n",
    "\n",
    "            # save the episode\n",
    "            all_logits.append(logits)\n",
    "            all_actions.append(a)\n",
    "            all_rewards.append(r)\n",
    "            \n",
    "            average_total_r += r\n",
    "        \n",
    "        all_actions = tf.stack(all_actions)\n",
    "        all_logits = tf.stack(all_logits)\n",
    "        all_rewards = np.array(all_rewards)\n",
    "        \n",
    "        # compute cummulative rewards for each action\n",
    "        all_cum_rewards = discount_rewards(all_rewards, all_actions.numpy())\n",
    "        all_cum_rewards -= np.mean(all_cum_rewards)\n",
    "        all_cum_rewards /= np.std(all_cum_rewards)\n",
    "        \n",
    "        loss_value = loss(all_logits, all_actions, all_cum_rewards)\n",
    "    \n",
    "    print(\"batch: {}, average_total_r: {}\".format(batch, np.mean(average_total_r)))\n",
    "    \n",
    "    grads = tape.gradient(loss_value, all_variables)\n",
    "    optimizer.apply_gradients(zip(grads, all_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
