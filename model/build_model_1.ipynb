{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update batch size\n",
    "batch_size = 100\n",
    "\n",
    "# number of batch in training\n",
    "num_of_batch = 1000\n",
    "\n",
    "# fixed number of time steps in one episode (not used)\n",
    "trading_period = 500\n",
    "\n",
    "# 1 is zscore\n",
    "num_features = 1\n",
    "\n",
    "# 0 is no position. 1 is long the spread. 2 is short the spread.\n",
    "a_num = position_num = 3\n",
    "\n",
    "# RNN hidden state dimension\n",
    "h_dim = 100\n",
    "\n",
    "# number of RNN layer\n",
    "num_layers = 1\n",
    "\n",
    "# number of layer1 output\n",
    "layer1_out_num = 50\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3\n",
    "\n",
    "# discount factor in reinforcement learning\n",
    "gamma = 1\n",
    "\n",
    "# random action probability\n",
    "rand_action_prob = 0.15\n",
    "\n",
    "batch_per_print = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_random_history(batch_size):\n",
    "    \"\"\"Sample some pairs and get the history of those pairs. The history should have\n",
    "    three dimension. The first dimension is for time. The second dimension is indexed\n",
    "    by features name. The third dimension is the index of training instance.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\"/home/u21376/fyp/dataset/GOOGL-GOOG-1.csv\")\n",
    "    history = df[['close1', 'close2', 'spread']].values\n",
    "    arr = []\n",
    "    for i in range(batch_size):\n",
    "        temp = history[i*trading_period:(i+1)*trading_period]\n",
    "        arr.append(history[i*trading_period:(i+1)*trading_period])\n",
    "    hist = np.array(arr)\n",
    "    return np.transpose(hist, (1, 2, 0))\n",
    "    \n",
    "\n",
    "def compute_input_history(history):\n",
    "    \"\"\"Slicing history in its second dimension.\"\"\"\n",
    "    return history[:,2:,:]\n",
    "\n",
    "def sample_action(logits, random=False):\n",
    "    if random:\n",
    "        dist = tf.distributions.Categorical(logits=tf.zeros([batch_size, a_num]))\n",
    "    else:\n",
    "        dist = tf.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    # 1-D Tensor where the i-th element correspond to a sample from\n",
    "    # the i-th categorical distribution\n",
    "    return dist.sample()\n",
    "\n",
    "def discount_rewards(r, all_actions):\n",
    "    \"\"\"\n",
    "    r is a numpy array in the shape of (n, batch_size).\n",
    "    all_actions is a numpy array in the same shape as r.\n",
    "    \n",
    "    return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "    result = np.zeros_like(r, dtype=float)\n",
    "    n = r.shape[0]\n",
    "    sum_ = np.zeros_like(r[0], dtype=float)\n",
    "    pre_action = all_actions[n-1]\n",
    "    for i in range(n-1,-1,-1):\n",
    "        sum_ *= gamma\n",
    "        \n",
    "        # when the previous action(position) not equal to the current one,\n",
    "        # set the previous sum of reward to be zero.\n",
    "        sum_ = sum_*(all_actions[i]==pre_action) + r[i]\n",
    "        result[i] = sum_\n",
    "        \n",
    "        # update pre_action\n",
    "        pre_action = all_actions[i]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def loss(all_logits, all_actions, all_advantages):\n",
    "    neg_log_select_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_logits, labels=all_actions)\n",
    "    \n",
    "    # 0 axis is the time axis. 1 axis is the batch axis\n",
    "    return tf.reduce_mean(neg_log_select_prob * all_advantages, 0)\n",
    "\n",
    "# classes\n",
    "class TradingPolicyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TradingPolicyModel, self).__init__()\n",
    "        self.dense1 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense2 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense3 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense4 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.logits = tf.layers.Dense(units=a_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        inputs = self.dense1(inputs)\n",
    "        inputs = self.dense2(inputs)\n",
    "        inputs = self.dense3(inputs)\n",
    "        inputs = self.dense4(inputs)\n",
    "        logits = self.logits(inputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class StateEncodingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(StateEncodingModel, self).__init__()\n",
    "        self.cell_layer = tf.contrib.rnn.LSTMCell(h_dim)\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell([self.cell_layer] * num_layers)\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        oberservation, self.state = self.cell(inputs, self.state)\n",
    "        return oberservation\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "class TradingEnvironment():\n",
    "    \"\"\"Trading environment for reinforcement learning training.\n",
    "    \n",
    "    Arguments:\n",
    "        state_encoding_model: the model that encode past input_history data into a state\n",
    "        vector which will be fed as input to the policy network.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_encoding_model):\n",
    "        # do some initialization\n",
    "        self.state_encoding_model = state_encoding_model\n",
    "        self._reset_env()\n",
    "        \n",
    "    def _reset_env(self):\n",
    "        self.t = 0\n",
    "        self.state_encoding_model.reset_state()\n",
    "\n",
    "        # 0 is no position. 1 is long the spread. 2 is short the spread\n",
    "        self.position = np.zeros(batch_size, dtype=int)\n",
    "\n",
    "        # prepare a batch of history and input_history\n",
    "        self.history = get_random_history(batch_size)\n",
    "        self.input_history = compute_input_history(self.history)\n",
    "        \n",
    "        # create or update self.state variable\n",
    "        self.update_state()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return an initial state for the trading environment\"\"\"\n",
    "        if self.t == 0:\n",
    "            return self.state\n",
    "        else:\n",
    "            self._reset_env()\n",
    "            return self.state\n",
    "    \n",
    "    def compute_reward(self, action):\n",
    "        # if action is 0, no reward.\n",
    "        # if action is 1 or 2, can compute immediate return as immediate reward\n",
    "        r = np.zeros_like(action, dtype=float)        \n",
    "        cur_his = self.history[self.t]\n",
    "        nex_his = self.history[self.t+1]\n",
    "        \n",
    "        # compute for each training instance in a batch\n",
    "        for i, a in enumerate(action):\n",
    "#             y_p = cur_his[\"y_close\"][i]\n",
    "#             x_p = cur_his[\"x_close\"][i]\n",
    "#             nex_y_p = nex_his[\"y_close\"][i]\n",
    "#             nex_x_p = nex_his[\"x_close\"][i]\n",
    "            \n",
    "            y_p = cur_his[0][i]\n",
    "            x_p = cur_his[1][i]\n",
    "            nex_y_p = nex_his[0][i]\n",
    "            nex_x_p = nex_his[1][i]\n",
    "            if a == 1:\n",
    "                r[i] = math.log(nex_y_p/y_p) + math.log(x_p/nex_x_p)\n",
    "            elif a == 2:\n",
    "                r[i] = math.log(nex_x_p/x_p) + math.log(y_p/nex_y_p)\n",
    "        return r\n",
    "    \n",
    "    def update_state(self):\n",
    "        # concate next_input_history and next position to form next partial state\n",
    "        partial_state = tf.concat([self.input_history[self.t].T, tf.one_hot(self.position, position_num)], 1)\n",
    "        \n",
    "        # update state\n",
    "        self.state = self.state_encoding_model(partial_state)        \n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Given the current state and action, return the reward, next state and done.\n",
    "        This function should be called after reset.\n",
    "        \n",
    "        reward is of type numpy array. state is of type tensor. done is of type boolean.\n",
    "        \n",
    "        \n",
    "        Arguments:\n",
    "            action: a numpy array containing the current action for each training pair.\n",
    "\n",
    "        Note that we follow the convention where the trajectory is indexed as s_0, a_0, r_0,\n",
    "        s_1, ... . Therefore t is updated just after computing the reward is computed and\n",
    "        before computing next state.\n",
    "        \"\"\"\n",
    "        # r_t\n",
    "        r = self.compute_reward(action)\n",
    "\n",
    "        # t = t+1\n",
    "        self.t += 1\n",
    "        \n",
    "        # compute s_(t+1)\n",
    "        self.position = action\n",
    "        self.update_state()\n",
    "\n",
    "        return r, self.state, self.t == batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 49, average_total_r_per_ep: 1.6464054135444144e-05\n",
      "batch: 99, average_total_r_per_ep: -2.2561814368062148e-05\n",
      "batch: 149, average_total_r_per_ep: -1.3817527085588554e-05\n",
      "batch: 199, average_total_r_per_ep: 5.936656222624392e-05\n",
      "batch: 249, average_total_r_per_ep: 2.0207143590096053e-05\n",
      "batch: 299, average_total_r_per_ep: 2.5382190553506685e-05\n",
      "batch: 349, average_total_r_per_ep: 5.874221155951589e-05\n",
      "batch: 399, average_total_r_per_ep: 3.5309044906404675e-05\n",
      "batch: 449, average_total_r_per_ep: 7.370314040504169e-05\n",
      "batch: 499, average_total_r_per_ep: 7.194255206300513e-05\n",
      "batch: 549, average_total_r_per_ep: 3.744918510306042e-05\n",
      "batch: 599, average_total_r_per_ep: 2.2135442501452317e-05\n",
      "batch: 649, average_total_r_per_ep: 4.44327209753318e-05\n",
      "batch: 699, average_total_r_per_ep: 5.214230257825284e-05\n",
      "batch: 749, average_total_r_per_ep: 1.8597701248665535e-05\n",
      "batch: 799, average_total_r_per_ep: 4.5274625407673044e-05\n",
      "batch: 849, average_total_r_per_ep: 3.1649892616269556e-05\n",
      "batch: 899, average_total_r_per_ep: 2.2087040420632587e-05\n",
      "batch: 949, average_total_r_per_ep: 2.69514208243513e-05\n",
      "batch: 999, average_total_r_per_ep: 3.282878433785706e-05\n"
     ]
    }
   ],
   "source": [
    "pi = TradingPolicyModel()\n",
    "state_encoding_model = StateEncodingModel()\n",
    "env = TradingEnvironment(state_encoding_model)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "# for training reference only\n",
    "average_total_r = np.zeros(batch_size)\n",
    "\n",
    "for batch in range(num_of_batch):\n",
    "    \n",
    "    # saving for update\n",
    "    all_logits = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    with tf.GradientTape() as gt:\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "\n",
    "        # internally the episode length is fixed by trading_period\n",
    "        while not done:\n",
    "            logits = pi(s)\n",
    "            a = sample_action(logits, random=np.random.rand() <= rand_action_prob)\n",
    "            r, next_s, done = env.step(a.numpy())\n",
    "\n",
    "            # save the episode\n",
    "            all_logits.append(logits)\n",
    "            all_actions.append(a)\n",
    "            all_rewards.append(r)\n",
    "            \n",
    "            average_total_r += r\n",
    "        \n",
    "        all_logits_stack = tf.stack(all_logits)\n",
    "        all_actions_stack = tf.stack(all_actions)\n",
    "        all_rewards_stack = np.array(all_rewards)\n",
    "        \n",
    "        # compute cummulative rewards for each action\n",
    "        all_cum_rewards = discount_rewards(all_rewards_stack, all_actions_stack.numpy())\n",
    "        all_cum_rewards -= np.mean(all_cum_rewards)\n",
    "        all_cum_rewards /= np.std(all_cum_rewards)\n",
    "        all_cum_rewards = tf.convert_to_tensor(all_cum_rewards, dtype=tf.float32)\n",
    "\n",
    "        loss_value = loss(all_logits_stack, all_actions_stack, all_cum_rewards)\n",
    "    \n",
    "    if (batch+1) % batch_per_print == 0:\n",
    "        print(\"batch: {}, average_total_r_per_ep: {}\".format(batch, np.mean(average_total_r)/batch_per_print))\n",
    "        average_total_r = np.zeros(batch_size)\n",
    "    \n",
    "    grads = gt.gradient(loss_value, state_encoding_model.variables + pi.variables)\n",
    "    optimizer.apply_gradients(zip(grads, state_encoding_model.variables + pi.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At test time, average_total_r_per_ep: 4.2436951307533045e-05\n"
     ]
    }
   ],
   "source": [
    "# test time\n",
    "average_total_r = np.zeros(batch_size)\n",
    "done = False\n",
    "s = env.reset()\n",
    "\n",
    "# internally the episode length is fixed by trading_period\n",
    "while not done:\n",
    "    logits = pi(s)\n",
    "    a = sample_action(logits)\n",
    "    r, next_s, done = env.step(a.numpy())\n",
    "\n",
    "    # save the episode\n",
    "    all_logits.append(logits)\n",
    "    all_actions.append(a)\n",
    "    all_rewards.append(r)\n",
    "\n",
    "    average_total_r += r\n",
    "\n",
    "print(\"At test time, average_total_r_per_ep: {}\".format(np.mean(average_total_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
