{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "# update batch size\n",
    "batch_size = 20\n",
    "\n",
    "# number of batch in training\n",
    "num_of_batch = 10\n",
    "\n",
    "# fixed number of time steps in one episode (not used)\n",
    "trading_period = 60\n",
    "\n",
    "# 1 is zscore\n",
    "num_features = 1\n",
    "\n",
    "# 0 is no position. 1 is long the spread. 2 is short the spread.\n",
    "position_num = 3\n",
    "\n",
    "# RNN hidden state dimension\n",
    "h_dim = 70\n",
    "\n",
    "# number of RNN layer\n",
    "num_layers = 1\n",
    "\n",
    "# number of actions\n",
    "a_num = 4\n",
    "\n",
    "# number of layer1 output\n",
    "layer1_out_num = 100\n",
    "\n",
    "# learning rate\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingPolicyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TradingPolicyModel, self).__init__()\n",
    "        self.dense1 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.logits = tf.layers.Dense(units=a_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        x = self.dense1(inputs)\n",
    "        logits = self.logits(x)\n",
    "        return logits\n",
    "\n",
    "def sample_action(logits):\n",
    "    dist = tf.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    # 1-D Tensor where the i-th element correspond to a sample from\n",
    "    # the i-th categorical distribution\n",
    "    return dist.sample().numpy()\n",
    "\n",
    "\n",
    "class StateEncodingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TradingPolicyModel, self).__init__()\n",
    "        self.cell_layer = tf.contrib.rnn.LSTMCell(h_dim)\n",
    "        sef.cell = tf.contrib.rnn.MultiRNNCell([self.cell_layer] * num_layers)\n",
    "        self.state = cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        oberservation, self.state = cell(inputs, self.state)\n",
    "        return oberservation\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "def get_random_history(history):\n",
    "    \"\"\"Sample some pairs and get the history of those pairs. The history should have\n",
    "    three dimension. The first dimension is for time. The second dimension is indexed\n",
    "    by features name. The third dimension is the index of training instance.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def compute_input_history(history):\n",
    "    \"\"\"Slicing history in its second dimension.\"\"\"\n",
    "    pass\n",
    "\n",
    "class TradingEnvironment():\n",
    "    \"\"\"Trading environment for reinforcement learning training.\n",
    "    \n",
    "    Arguments:\n",
    "        state_encoding_model: the model that encode past input_history data into a state\n",
    "        vector which will be fed as input to the policy network.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_encoding_model):\n",
    "        # do some initialization\n",
    "        self.state_encoding_model = state_encoding_model\n",
    "        self._reset_env()\n",
    "        \n",
    "    def _reset_env(self):\n",
    "        self.t = 0\n",
    "        self.rnn_state = self.state_encoding_model.zero_state()\n",
    "\n",
    "        # 0 is no position. 1 is long the spread. 2 is short the spread\n",
    "        self.position = np.zeros(batch_size)\n",
    "\n",
    "        # prepare a batch of history and input_history\n",
    "        self.history = get_random_history(batch_size)\n",
    "        self.input_history = compute_input_history(self.history)\n",
    "        \n",
    "        # create or update self.state variable\n",
    "        self.update_state()\n",
    "        \n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return an initial state for the trading environment\"\"\"\n",
    "        if self.t == 0:\n",
    "            return self.state\n",
    "        else:\n",
    "            self._reset_env()\n",
    "            return self.state\n",
    "    \n",
    "    def reward_magic_func(self):\n",
    "        pass\n",
    "    \n",
    "    def update_state(self):\n",
    "        # concate next_input_history and next position to form next partial state\n",
    "        partial_state = tf.concat([self.input_history[self.t], tf.one_hot(self.position, position_num)], 1)\n",
    "        \n",
    "        # update state\n",
    "        self.state, self.rnn_state = self.state_encode_model(partial_state, self.rnn_state)\n",
    "    \n",
    "    def compute_next_state(self, action):\n",
    "        # compute_next_position\n",
    "        next_pos = np.zeros_like(self.position)\n",
    "        next_pos += (action == 0)*self.position\n",
    "        next_pos += (action == 1)*1\n",
    "        next_pos += (action == 2)*2\n",
    "        # this line has no effect as next_pos is initialized as zero\n",
    "#         next_pos += (action == 3)*0\n",
    "        self.position = next_pos\n",
    "        \n",
    "        self.update_state()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Given the current state and action, return the reward, next state and done.\n",
    "        This function should be called after reset.\n",
    "        \n",
    "        \n",
    "        Arguments:\n",
    "            action: a numpy array containing the current action for each training pair.\n",
    "\n",
    "        Note that we follow the convention where the trajectory is indexed as s_0, a_0, r_0,\n",
    "        s_1, ... . Therefore t is updated just after computing the reward is computed and\n",
    "        before computing next state.\n",
    "        \"\"\"\n",
    "        r = self.reward_magic_func()\n",
    "        self.t += 1\n",
    "        self.compute_next_state(action)\n",
    "        return r, self.state, self.t == batch_size\n",
    "\n",
    "def loss(all_logits, all_actions, all_advantages):\n",
    "    neg_log_select_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_logits, labels=all_actions)\n",
    "    \n",
    "    # 0 axis is the time axis. 1 axis is the batch axis\n",
    "    return tf.reduce_mean(neg_log_select_prob * all_advantages, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = TradingPolicyModel()\n",
    "state_encode_model = StateEncodingModel()\n",
    "env = TradingEnvironment(state_encode_model)\n",
    "\n",
    "for batch in range(num_of_batch):\n",
    "\n",
    "    all_logits = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    with tf.GradientTape() as gt:\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "\n",
    "        # internally the episode length is fixed by trading_period\n",
    "        while not done:\n",
    "            logits = pi(0)\n",
    "            a = sample_action(logits)\n",
    "            r, next_s, done = senv.step(a.numpy())\n",
    "        \n",
    "            all_logits.append(logits)\n",
    "            all_actions.append(a)\n",
    "            all_rewards.append(r)\n",
    "        \n",
    "        loss(tf.stack(all_logits), tf.stack(all_actions), )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
