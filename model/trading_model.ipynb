{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from os.path import isfile, join, splitext\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from process_raw_prices import *\n",
    "\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/u21376/fyp/code/model\n"
     ]
    }
   ],
   "source": [
    "# make sure the jupyter notebook run in this working directory\n",
    "%cd ~/fyp/code/model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_pair = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pair slices: 26680\n",
      "Total number of pair slices for training: 1000\n",
      "Total number of pair slices for testing: 1000\n"
     ]
    }
   ],
   "source": [
    "# processed dataset folder path\n",
    "dataset_folder_path = '../../dataset/nyse-daily-transformed'\n",
    "os.makedirs(dataset_folder_path, exist_ok=True)\n",
    "\n",
    "# raw dataset files pattern\n",
    "raw_files_path_pattern = \"../../dataset/nyse-daily-trimmed-same-length/*.csv\"\n",
    "\n",
    "df_columns = ['close1', 'close2', 'normalizedLogClose1', 'normalizedLogClose2', 'spread', 'alpha', 'beta']\n",
    "ind = {'y_close': 0, 'x_close': 1, 'spread': 4}\n",
    "\n",
    "# compute dataset for training\n",
    "all_pairs_slices = [splitext(f)[0] for f in os.listdir(dataset_folder_path) if isfile(join(dataset_folder_path, f))]\n",
    "if len(all_pairs_slices) == 0:\n",
    "    generate_pairs_training_data(raw_files_path_pattern=raw_files_path_pattern,\n",
    "                                 result_path=dataset_folder_path,\n",
    "                                 min_size=252*4,\n",
    "                                 training_period=52,\n",
    "                                 points_per_cut=252\n",
    "                                )\n",
    "    all_pairs_slices = [splitext(f)[0] for f in os.listdir(dataset_folder_path) if isfile(join(dataset_folder_path, f))]\n",
    "print(\"Total number of pair slices: %d\" % len(all_pairs_slices))\n",
    "\n",
    "# split for training and testing\n",
    "all_pairs = sorted(list(set(['-'.join(p.split('-')[0:2]) for p in all_pairs_slices])))[:num_of_pair]\n",
    "# all_pairs = [\"VMW-WUBA\"]\n",
    "# all_pairs = [\"TWTR-UIS\"]\n",
    "all_pairs_slices_train = []\n",
    "all_pairs_slices_test = []\n",
    "for p in all_pairs:\n",
    "    all_pairs_slices_train += [p+'-0', p+'-1']\n",
    "    all_pairs_slices_test += [p+'-2', p+'-3']\n",
    "print(\"Total number of pair slices for training: %d\" % len(all_pairs_slices_train))\n",
    "print(\"Total number of pair slices for testing: %d\" % len(all_pairs_slices_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAN-AER', 'AAN-AL', 'AAN-AMBR', 'AAN-AMN', 'AAN-ANET', 'AAN-ASGN', 'AAN-ATEN', 'AAN-ATHM', 'AAN-ATU', 'AAN-AUO', 'AAN-AYR', 'AAN-BHE', 'AAN-BITA', 'AAN-CACI', 'AAN-CAI', 'AAN-CLGX', 'AAN-CLS', 'AAN-CMCM', 'AAN-CRM', 'AAN-CSLT', 'AAN-CTS', 'AAN-CW', 'AAN-DATA', 'AAN-DDD', 'AAN-DOV', 'AAN-DQ', 'AAN-ECOM', 'AAN-EGHT', 'AAN-ELLI', 'AAN-EPAM', 'AAN-ETN', 'AAN-EVTC', 'AAN-FDS', 'AAN-GHM', 'AAN-GLOB', 'AAN-GWRE', 'AAN-HHS', 'AAN-HIVE', 'AAN-HPQ', 'AAN-HUBS', 'AAN-IBM', 'AAN-INFY', 'AAN-INXN', 'AAN-IPG', 'AAN-IPHI', 'AAN-ITW', 'AAN-JBL', 'AAN-JBT', 'AAN-JKS', 'AAN-JNPR', 'AAN-KAI', 'AAN-KFY', 'AAN-LDOS', 'AAN-LEAF', 'AAN-LLL', 'AAN-LXFT', 'AAN-MAN', 'AAN-MIXT', 'AAN-MODN', 'AAN-MSI', 'AAN-MX', 'AAN-MXL', 'AAN-NEWR', 'AAN-NOK', 'AAN-NOW', 'AAN-NPTN', 'AAN-NSP', 'AAN-OMC', 'AAN-ORCL', 'AAN-PANW', 'AAN-PAYC', 'AAN-PKE', 'AAN-PNR', 'AAN-PRO', 'AAN-QTM', 'AAN-QTWO', 'AAN-RENN', 'AAN-RHI', 'AAN-RHT', 'AAN-RNG', 'AAN-RST', 'AAN-RUBI', 'AAN-RXN', 'AAN-SAIC', 'AAN-SAP', 'AAN-SFUN', 'AAN-SMI', 'AAN-SNX', 'AAN-SOL', 'AAN-SPA', 'AAN-SPXC', 'AAN-SQNS', 'AAN-SRT', 'AAN-SSTK', 'AAN-STM', 'AAN-SXI', 'AAN-TBI', 'AAN-TDC', 'AAN-TGH', 'AAN-TLRA', 'AAN-TNC', 'AAN-TRTN', 'AAN-TSM', 'AAN-TWTR', 'AAN-TYL', 'AAN-UIS', 'AAN-UMC', 'AAN-URI', 'AAN-VEEV', 'AAN-VMW', 'AAN-WIT', 'AAN-WK', 'AAN-WUBA', 'AAN-XRX', 'AAN-ZEN', 'AER-AL', 'AER-AMBR', 'AER-AMN', 'AER-ANET', 'AER-ASGN', 'AER-ATEN', 'AER-ATHM', 'AER-ATU', 'AER-AUO', 'AER-AYR', 'AER-BHE', 'AER-BITA', 'AER-CACI', 'AER-CAI', 'AER-CLGX', 'AER-CLS', 'AER-CMCM', 'AER-CRM', 'AER-CSLT', 'AER-CTS', 'AER-CW', 'AER-DATA', 'AER-DDD', 'AER-DOV', 'AER-DQ', 'AER-ECOM', 'AER-EGHT', 'AER-ELLI', 'AER-EPAM', 'AER-ETN', 'AER-EVTC', 'AER-FDS', 'AER-GHM', 'AER-GLOB', 'AER-GWRE', 'AER-HHS', 'AER-HIVE', 'AER-HPQ', 'AER-HUBS', 'AER-IBM', 'AER-INFY', 'AER-INXN', 'AER-IPG', 'AER-IPHI', 'AER-ITW', 'AER-JBL', 'AER-JBT', 'AER-JKS', 'AER-JNPR', 'AER-KAI', 'AER-KFY', 'AER-LDOS', 'AER-LEAF', 'AER-LLL', 'AER-LXFT', 'AER-MAN', 'AER-MIXT', 'AER-MODN', 'AER-MSI', 'AER-MX', 'AER-MXL', 'AER-NEWR', 'AER-NOK', 'AER-NOW', 'AER-NPTN', 'AER-NSP', 'AER-OMC', 'AER-ORCL', 'AER-PANW', 'AER-PAYC', 'AER-PKE', 'AER-PNR', 'AER-PRO', 'AER-QTM', 'AER-QTWO', 'AER-RENN', 'AER-RHI', 'AER-RHT', 'AER-RNG', 'AER-RST', 'AER-RUBI', 'AER-RXN', 'AER-SAIC', 'AER-SAP', 'AER-SFUN', 'AER-SMI', 'AER-SNX', 'AER-SOL', 'AER-SPA', 'AER-SPXC', 'AER-SQNS', 'AER-SRT', 'AER-SSTK', 'AER-STM', 'AER-SXI', 'AER-TBI', 'AER-TDC', 'AER-TGH', 'AER-TLRA', 'AER-TNC', 'AER-TRTN', 'AER-TSM', 'AER-TWTR', 'AER-TYL', 'AER-UIS', 'AER-UMC', 'AER-URI', 'AER-VEEV', 'AER-VMW', 'AER-WIT', 'AER-WK', 'AER-WUBA', 'AER-XRX', 'AER-ZEN', 'AL-AMBR', 'AL-AMN', 'AL-ANET', 'AL-ASGN', 'AL-ATEN', 'AL-ATHM', 'AL-ATU', 'AL-AUO', 'AL-AYR', 'AL-BHE', 'AL-BITA', 'AL-CACI', 'AL-CAI', 'AL-CLGX', 'AL-CLS', 'AL-CMCM', 'AL-CRM', 'AL-CSLT', 'AL-CTS', 'AL-CW', 'AL-DATA', 'AL-DDD', 'AL-DOV', 'AL-DQ', 'AL-ECOM', 'AL-EGHT', 'AL-ELLI', 'AL-EPAM', 'AL-ETN', 'AL-EVTC', 'AL-FDS', 'AL-GHM', 'AL-GLOB', 'AL-GWRE', 'AL-HHS', 'AL-HIVE', 'AL-HPQ', 'AL-HUBS', 'AL-IBM', 'AL-INFY', 'AL-INXN', 'AL-IPG', 'AL-IPHI', 'AL-ITW', 'AL-JBL', 'AL-JBT', 'AL-JKS', 'AL-JNPR', 'AL-KAI', 'AL-KFY', 'AL-LDOS', 'AL-LEAF', 'AL-LLL', 'AL-LXFT', 'AL-MAN', 'AL-MIXT', 'AL-MODN', 'AL-MSI', 'AL-MX', 'AL-MXL', 'AL-NEWR', 'AL-NOK', 'AL-NOW', 'AL-NPTN', 'AL-NSP', 'AL-OMC', 'AL-ORCL', 'AL-PANW', 'AL-PAYC', 'AL-PKE', 'AL-PNR', 'AL-PRO', 'AL-QTM', 'AL-QTWO', 'AL-RENN', 'AL-RHI', 'AL-RHT', 'AL-RNG', 'AL-RST', 'AL-RUBI', 'AL-RXN', 'AL-SAIC', 'AL-SAP', 'AL-SFUN', 'AL-SMI', 'AL-SNX', 'AL-SOL', 'AL-SPA', 'AL-SPXC', 'AL-SQNS', 'AL-SRT', 'AL-SSTK', 'AL-STM', 'AL-SXI', 'AL-TBI', 'AL-TDC', 'AL-TGH', 'AL-TLRA', 'AL-TNC', 'AL-TRTN', 'AL-TSM', 'AL-TWTR', 'AL-TYL', 'AL-UIS', 'AL-UMC', 'AL-URI', 'AL-VEEV', 'AL-VMW', 'AL-WIT', 'AL-WK', 'AL-WUBA', 'AL-XRX', 'AL-ZEN', 'AMBR-AMN', 'AMBR-ANET', 'AMBR-ASGN', 'AMBR-ATEN', 'AMBR-ATHM', 'AMBR-ATU', 'AMBR-AUO', 'AMBR-AYR', 'AMBR-BHE', 'AMBR-BITA', 'AMBR-CACI', 'AMBR-CAI', 'AMBR-CLGX', 'AMBR-CLS', 'AMBR-CMCM', 'AMBR-CRM', 'AMBR-CSLT', 'AMBR-CTS', 'AMBR-CW', 'AMBR-DATA', 'AMBR-DDD', 'AMBR-DOV', 'AMBR-DQ', 'AMBR-ECOM', 'AMBR-EGHT', 'AMBR-ELLI', 'AMBR-EPAM', 'AMBR-ETN', 'AMBR-EVTC', 'AMBR-FDS', 'AMBR-GHM', 'AMBR-GLOB', 'AMBR-GWRE', 'AMBR-HHS', 'AMBR-HIVE', 'AMBR-HPQ', 'AMBR-HUBS', 'AMBR-IBM', 'AMBR-INFY', 'AMBR-INXN', 'AMBR-IPG', 'AMBR-IPHI', 'AMBR-ITW', 'AMBR-JBL', 'AMBR-JBT', 'AMBR-JKS', 'AMBR-JNPR', 'AMBR-KAI', 'AMBR-KFY', 'AMBR-LDOS', 'AMBR-LEAF', 'AMBR-LLL', 'AMBR-LXFT', 'AMBR-MAN', 'AMBR-MIXT', 'AMBR-MODN', 'AMBR-MSI', 'AMBR-MX', 'AMBR-MXL', 'AMBR-NEWR', 'AMBR-NOK', 'AMBR-NOW', 'AMBR-NPTN', 'AMBR-NSP', 'AMBR-OMC', 'AMBR-ORCL', 'AMBR-PANW', 'AMBR-PAYC', 'AMBR-PKE', 'AMBR-PNR', 'AMBR-PRO', 'AMBR-QTM', 'AMBR-QTWO', 'AMBR-RENN', 'AMBR-RHI', 'AMBR-RHT', 'AMBR-RNG', 'AMBR-RST', 'AMBR-RUBI', 'AMBR-RXN', 'AMBR-SAIC', 'AMBR-SAP', 'AMBR-SFUN', 'AMBR-SMI', 'AMBR-SNX', 'AMBR-SOL', 'AMBR-SPA', 'AMBR-SPXC', 'AMBR-SQNS', 'AMBR-SRT', 'AMBR-SSTK', 'AMBR-STM', 'AMBR-SXI', 'AMBR-TBI', 'AMBR-TDC', 'AMBR-TGH', 'AMBR-TLRA', 'AMBR-TNC', 'AMBR-TRTN', 'AMBR-TSM', 'AMBR-TWTR', 'AMBR-TYL', 'AMBR-UIS', 'AMBR-UMC', 'AMBR-URI', 'AMBR-VEEV', 'AMBR-VMW', 'AMBR-WIT', 'AMBR-WK', 'AMBR-WUBA', 'AMBR-XRX', 'AMBR-ZEN', 'AMN-ANET', 'AMN-ASGN', 'AMN-ATEN', 'AMN-ATHM', 'AMN-ATU', 'AMN-AUO', 'AMN-AYR', 'AMN-BHE', 'AMN-BITA', 'AMN-CACI', 'AMN-CAI', 'AMN-CLGX', 'AMN-CLS', 'AMN-CMCM', 'AMN-CRM', 'AMN-CSLT', 'AMN-CTS', 'AMN-CW', 'AMN-DATA', 'AMN-DDD', 'AMN-DOV', 'AMN-DQ', 'AMN-ECOM', 'AMN-EGHT', 'AMN-ELLI', 'AMN-EPAM', 'AMN-ETN', 'AMN-EVTC', 'AMN-FDS', 'AMN-GHM', 'AMN-GLOB', 'AMN-GWRE', 'AMN-HHS', 'AMN-HIVE', 'AMN-HPQ', 'AMN-HUBS', 'AMN-IBM', 'AMN-INFY', 'AMN-INXN', 'AMN-IPG', 'AMN-IPHI', 'AMN-ITW', 'AMN-JBL', 'AMN-JBT', 'AMN-JKS', 'AMN-JNPR']\n"
     ]
    }
   ],
   "source": [
    "print(all_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update batch size\n",
    "batch_size = 128\n",
    "\n",
    "# number of batch in training\n",
    "num_of_batch = num_of_pair*500//batch_size\n",
    "\n",
    "# fixed number of time steps in one episode (not used)\n",
    "trading_period = 200\n",
    "\n",
    "# # 1 is zscore\n",
    "# num_features = 1\n",
    "\n",
    "# 0 is no position. 1 is long the spread. 2 is short the spread.\n",
    "a_num = position_num = 3\n",
    "\n",
    "# RNN hidden state dimension\n",
    "h_dim = 30\n",
    "\n",
    "# number of RNN layer\n",
    "num_layers = 2\n",
    "\n",
    "# number of layer1 output\n",
    "layer1_out_num = 30\n",
    "\n",
    "# learning rate\n",
    "lr = 5e-3\n",
    "\n",
    "reg = 0.001\n",
    "\n",
    "# discount factor in reinforcement learning\n",
    "gamma = 1\n",
    "\n",
    "# random action probability\n",
    "rand_action_prob = 0.08\n",
    "\n",
    "batch_per_print = 10\n",
    "\n",
    "# dummy initial cash\n",
    "initial_cash = 10000\n",
    "\n",
    "# checkpoint folder\n",
    "checkpoint_dir = '../../model_checkpoint/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_random_history(batch_size, training):\n",
    "    \"\"\"Sample some pairs and get the history of those pairs. The history should have\n",
    "    three dimension. The first dimension is for time. The second dimension is indexed\n",
    "    by features name. The third dimension is the index of training instance.\n",
    "    \"\"\"\n",
    "    sample_pair_slices = random.sample(all_pairs_slices_train if training else all_pairs_slices_test, batch_size)\n",
    "    history = []\n",
    "    for s in sample_pair_slices:\n",
    "        df = pd.read_csv(join(dataset_folder_path, s+\".csv\"))\n",
    "        df_val = df[df_columns].values\n",
    "        history.append(df_val)\n",
    "    \n",
    "    history = np.array(history)\n",
    "    return np.transpose(history, (1, 2, 0))\n",
    "\n",
    "def compute_input_history(history):\n",
    "    \"\"\"Slicing history in its second dimension.\"\"\"\n",
    "    # no slicing for now\n",
    "    return history[:,2:]\n",
    "\n",
    "def sample_action(logits, random=False):\n",
    "    if random:\n",
    "        dist = tf.distributions.Categorical(logits=tf.zeros([batch_size, a_num]))\n",
    "    else:\n",
    "        dist = tf.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    # 1-D Tensor where the i-th element correspond to a sample from\n",
    "    # the i-th categorical distribution\n",
    "    return dist.sample()\n",
    "\n",
    "def long_portfolio_value(q, p):\n",
    "    return q*p\n",
    "\n",
    "def short_portfolio_value(q, p, init_p):\n",
    "    return q*(3.0*init_p/2 - p)\n",
    "\n",
    "# def discount_rewards(r, all_actions):\n",
    "#     \"\"\"\n",
    "#     r is a numpy array in the shape of (n, batch_size).\n",
    "#     all_actions is a numpy array in the same shape as r.\n",
    "    \n",
    "#     return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "#     result = np.zeros_like(r, dtype=float)\n",
    "#     n = r.shape[0]\n",
    "#     sum_ = np.zeros_like(r[0], dtype=float)\n",
    "#     pre_action = all_actions[n-1]\n",
    "#     for i in range(n-1,-1,-1):\n",
    "#         sum_ *= gamma\n",
    "        \n",
    "#         # when the previous action(position) not equal to the current one,\n",
    "#         # set the previous sum of reward to be zero.\n",
    "#         sum_ = sum_*(all_actions[i]==pre_action) + r[i]\n",
    "#         result[i] = sum_\n",
    "        \n",
    "#         # update pre_action\n",
    "#         pre_action = all_actions[i]\n",
    "    \n",
    "#     return result\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\"\n",
    "    r is a numpy array in the shape of (n, batch_size).\n",
    "    \n",
    "    return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "    result = np.zeros_like(r, dtype=float)\n",
    "    n = r.shape[0]\n",
    "    sum_ = np.zeros_like(r[0], dtype=float)\n",
    "    for i in range(n-1,-1,-1):\n",
    "        sum_ *= gamma\n",
    "        sum_ += r[i]\n",
    "        result[i] = sum_\n",
    "    \n",
    "    return result\n",
    "\n",
    "def loss(all_logits, all_actions, all_advantages):\n",
    "    neg_log_select_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_logits, labels=all_actions)\n",
    "    \n",
    "    # 0 axis is the time axis. 1 axis is the batch axis\n",
    "    return tf.reduce_mean(neg_log_select_prob * all_advantages, 0)\n",
    "\n",
    "def save_model():\n",
    "    hkg_time = datetime.now() + timedelta(hours=16)\n",
    "    checkpoint_name = hkg_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    root.save(checkpoint_prefix)\n",
    "    tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "def restore_model(checkpoint_name):\n",
    "    root.restore(join(checkpoint_dir, checkpoint_name))\n",
    "\n",
    "\n",
    "myLeakyReLU = tf.keras.layers.LeakyReLU()\n",
    "myLeakyReLU.__name__ = \"myLeakyReLU\"\n",
    "\n",
    "# classes\n",
    "class TradingPolicyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TradingPolicyModel, self).__init__()\n",
    "        self.dense1 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(reg)\n",
    "                                     )\n",
    "        self.dense2 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(reg)\n",
    "                                     )\n",
    "        self.dense3 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(reg)\n",
    "                                     )\n",
    "#         self.dense4 = tf.layers.Dense(units=layer1_out_num,\n",
    "#                                       activation=tf.keras.layers.LeakyReLU(),\n",
    "#                                       kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "#                                      )\n",
    "        self.logits = tf.layers.Dense(units=a_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                      kernel_regularizer=tf.contrib.layers.l2_regularizer(reg)\n",
    "                                     )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        inputs = self.dense1(inputs)\n",
    "        inputs = self.dense2(inputs)\n",
    "        inputs = self.dense3(inputs)\n",
    "#         inputs = self.dense4(inputs)\n",
    "        logits = self.logits(inputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class StateEncodingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(StateEncodingModel, self).__init__()\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell([tf.contrib.rnn.LSTMCell(h_dim) for i in range(num_layers)])\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        output, self.state = self.cell(inputs, self.state)\n",
    "        return output\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "class TradingEnvironment():\n",
    "    \"\"\"Trading environment for reinforcement learning training.\n",
    "    \n",
    "    Arguments:\n",
    "        state_encoding_model: the model that encode past input_history data into a state\n",
    "        vector which will be fed as input to the policy network.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_encoding_model):\n",
    "        # do some initialization\n",
    "        self.state_encoding_model = state_encoding_model\n",
    "        \n",
    "        # use training dataset\n",
    "        self._reset_env(training=True)\n",
    "        \n",
    "    def _reset_env(self, training=True):\n",
    "        self.t = 0\n",
    "        self.state_encoding_model.reset_state()\n",
    "\n",
    "        # 0 is no position. 1 is long the spread. 2 is short the spread\n",
    "        self.position = np.zeros(batch_size, dtype=int)\n",
    "        \n",
    "        # initialize the cash each agent has\n",
    "        self.total_portfolio_value = np.ones(batch_size)*initial_cash\n",
    "        \n",
    "        # only useful when there is a postion on the spread\n",
    "        self.quantity = {'x': np.zeros(batch_size), 'y': np.zeros(batch_size)}\n",
    "        \n",
    "        # for compute current portfolio value of the short side\n",
    "        self.short_side_init_price = np.zeros(batch_size)\n",
    "\n",
    "        # prepare a batch of history and input_history\n",
    "        self.history = get_random_history(batch_size, training)\n",
    "        self.input_history = compute_input_history(self.history)\n",
    "        \n",
    "        # create or update self.state variable\n",
    "        self.update_state()\n",
    "    \n",
    "    def reset(self, training=True):\n",
    "        \"\"\"Return an initial state for the trading environment\"\"\"\n",
    "        \n",
    "        # determine what dataset to use\n",
    "        self._reset_env(training=training)\n",
    "        return self.state\n",
    "    \n",
    "    def compute_reward(self, action):\n",
    "        \"\"\"Compute the reward at time t which is the change in total portfolio value\n",
    "        from time t to t+1. It also update the position for time t+1. Exit trade when\n",
    "        the short side portfolio value <= 0.\"\"\"\n",
    "        \n",
    "        r = np.zeros_like(action, dtype=float)\n",
    "        cur_his = self.history[self.t]\n",
    "        nex_his = self.history[self.t+1]\n",
    "        \n",
    "        # compute for each training instance in a batch\n",
    "        for i, a in enumerate(action):\n",
    "            y_p = cur_his[ind[\"y_close\"], i]\n",
    "            x_p = cur_his[ind[\"x_close\"], i]\n",
    "            nex_y_p = nex_his[ind[\"y_close\"], i]\n",
    "            nex_x_p = nex_his[ind[\"x_close\"], i]\n",
    "            \n",
    "            \n",
    "            if a == 0: # take no position on the spread\n",
    "                # no change in portfolio value\n",
    "                r[i] = 0\n",
    "                self.position[i] = 0\n",
    "                self.quantity['y'][i] = 0.0\n",
    "                self.quantity['x'][i] = 0.0\n",
    "            elif a == 1: # long the spread: long Y and short X\n",
    "                # quantity of each stock will change when the current position is not previous position\n",
    "                if self.position[i] == 0 or self.position[i] == 2:\n",
    "                    # compute quantity from cash\n",
    "                    self.quantity['y'][i] = 2.0*self.total_portfolio_value[i]/3.0/y_p\n",
    "                    self.quantity['x'][i] = 2.0*self.total_portfolio_value[i]/3.0/x_p\n",
    "                    self.short_side_init_price[i] = x_p\n",
    "\n",
    "                lpv = long_portfolio_value(self.quantity['y'][i], nex_y_p)\n",
    "                spv = short_portfolio_value(self.quantity['x'][i], nex_x_p, self.short_side_init_price[i])\n",
    "                \n",
    "                # the zero here can be changed to other positive threshold ...\n",
    "                if spv <= 0:\n",
    "                    # we loss all the money in the short side\n",
    "                    nex_portfolio_value = lpv\n",
    "\n",
    "                    # forced to take position 0\n",
    "                    self.position[i] = 0\n",
    "                else:\n",
    "                    nex_portfolio_value = lpv + spv\n",
    "                    self.position[i] = 1\n",
    "                \n",
    "                r[i] = nex_portfolio_value - self.total_portfolio_value[i]\n",
    "                self.total_portfolio_value[i] = nex_portfolio_value\n",
    "            elif a == 2: # short the spread: short Y and long X\n",
    "                # quantity will change when the current position is not previous position\n",
    "                if self.position[i] == 0 or self.position[i] == 1:\n",
    "                    # compute quantity from cash\n",
    "                    self.quantity['y'][i] = 2.0*self.total_portfolio_value[i]/3.0/y_p\n",
    "                    self.quantity['x'][i] = 2.0*self.total_portfolio_value[i]/3.0/x_p\n",
    "                    self.short_side_init_price[i] = y_p\n",
    "\n",
    "                lpv = long_portfolio_value(self.quantity['x'][i], nex_x_p)\n",
    "                spv = short_portfolio_value(self.quantity['y'][i], nex_y_p, self.short_side_init_price[i])\n",
    "                \n",
    "                if spv <= 0:\n",
    "                    # we loss all the money in the short side\n",
    "                    nex_portfolio_value = lpv\n",
    "\n",
    "                    # forced to take position 0\n",
    "                    self.position[i] = 0\n",
    "                else:\n",
    "                    nex_portfolio_value = lpv + spv\n",
    "                    self.position[i] = 2\n",
    "                \n",
    "                r[i] = nex_portfolio_value - self.total_portfolio_value[i]\n",
    "                self.total_portfolio_value[i] = nex_portfolio_value\n",
    "        return r\n",
    "    \n",
    "    def update_state(self):\n",
    "#         # concate next_input_history and next position to form next partial state\n",
    "#         partial_state = tf.concat([self.input_history[self.t].T, tf.one_hot(self.position, position_num)], 1)\n",
    "        \n",
    "#         # update state\n",
    "#         self.state = self.state_encoding_model(partial_state)\n",
    "\n",
    "        observation = tf.convert_to_tensor(self.input_history[self.t].T, dtype=tf.float32)\n",
    "    \n",
    "        # use rnn to encode observationans and current stock state into next stock state\n",
    "        stock_state = self.state_encoding_model(observation)\n",
    "        \n",
    "        portfolio_state = np.array([\n",
    "            self.total_portfolio_value,\n",
    "#             self.quantity['y'],\n",
    "#             self.quantity['x']\n",
    "        ]).T\n",
    "        \n",
    "        # stock state and portfolio state together form the whole environment state\n",
    "        self.state = tf.concat([\n",
    "            stock_state,\n",
    "            portfolio_state,\n",
    "            tf.one_hot(self.position, position_num)\n",
    "        ], 1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Given the current state and action, return the reward, next state and done.\n",
    "        This function should be called after reset.\n",
    "        \n",
    "        reward is of type numpy array. state is of type tensor. done is of type boolean.\n",
    "        \n",
    "        \n",
    "        Arguments:\n",
    "            action: a numpy array containing the current action for each training pair.\n",
    "\n",
    "        Note that we follow the convention where the trajectory is indexed as s_0, a_0, r_0,\n",
    "        s_1, ... . Therefore t is updated just after computing the reward is computed and\n",
    "        before computing next state.\n",
    "        \"\"\"\n",
    "        # r_t\n",
    "        r = self.compute_reward(action) # also update the position for time t+1\n",
    "\n",
    "        # t = t+1\n",
    "        self.t += 1\n",
    "        \n",
    "        # compute s_(t+1)\n",
    "        self.update_state()\n",
    "\n",
    "        return r, self.state, (self.t+1) == trading_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects\n",
    "pi = TradingPolicyModel()\n",
    "state_encoding_model = StateEncodingModel()\n",
    "env = TradingEnvironment(state_encoding_model)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "# create checkpoint object\n",
    "root = tf.train.Checkpoint(pi=pi, state_encoding_model=state_encoding_model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average_total_r = np.zeros(batch_size)\n",
    "# done = False\n",
    "# s = env.reset(training=True)\n",
    "\n",
    "# with tf.GradientTape() as gt:\n",
    "#     done = False\n",
    "#     s = env.reset(training=True)\n",
    "\n",
    "#     # internally the episode length is fixed by trading_period\n",
    "#     while not done:\n",
    "#         logits = pi(s)\n",
    "#         a = sample_action(logits)\n",
    "#         r, next_s, done = env.step(a.numpy())\n",
    "#         average_total_r += r\n",
    "# print('total_r_per_ep: {}'.format(np.mean(average_total_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_pair = 500\n",
      "batch_size = 128\n",
      "num_of_batch = 1953, estimated epoch = 249.984\n",
      "rand_action_prob = 0.08\n",
      "lr = 0.005\n",
      "batch id: 9, average_total_r_per_ep: 4.25, time spent per 10 batches: 16.6s\n",
      "batch id: 19, average_total_r_per_ep: -24.66, time spent per 10 batches: 16.4s\n",
      "batch id: 29, average_total_r_per_ep: 17.54, time spent per 10 batches: 16.2s\n",
      "batch id: 39, average_total_r_per_ep: -6.29, time spent per 10 batches: 16.2s\n",
      "batch id: 49, average_total_r_per_ep: 26.88, time spent per 10 batches: 16.0s\n",
      "batch id: 59, average_total_r_per_ep: -105.42, time spent per 10 batches: 17.0s\n",
      "batch id: 69, average_total_r_per_ep: -157.77, time spent per 10 batches: 17.3s\n",
      "batch id: 79, average_total_r_per_ep: -162.64, time spent per 10 batches: 17.6s\n",
      "batch id: 89, average_total_r_per_ep: -15.93, time spent per 10 batches: 16.1s\n",
      "batch id: 99, average_total_r_per_ep: -15.12, time spent per 10 batches: 15.9s\n",
      "batch id: 109, average_total_r_per_ep: 20.70, time spent per 10 batches: 16.6s\n",
      "batch id: 119, average_total_r_per_ep: 1.02, time spent per 10 batches: 16.8s\n",
      "batch id: 129, average_total_r_per_ep: 3.93, time spent per 10 batches: 16.0s\n",
      "batch id: 139, average_total_r_per_ep: -37.52, time spent per 10 batches: 16.0s\n",
      "batch id: 149, average_total_r_per_ep: -6.48, time spent per 10 batches: 16.0s\n",
      "batch id: 159, average_total_r_per_ep: -13.80, time spent per 10 batches: 15.7s\n",
      "batch id: 169, average_total_r_per_ep: -31.52, time spent per 10 batches: 16.0s\n",
      "batch id: 179, average_total_r_per_ep: -9.58, time spent per 10 batches: 15.9s\n",
      "batch id: 189, average_total_r_per_ep: -10.05, time spent per 10 batches: 15.7s\n",
      "batch id: 199, average_total_r_per_ep: -5.24, time spent per 10 batches: 15.8s\n",
      "batch id: 209, average_total_r_per_ep: -39.08, time spent per 10 batches: 16.0s\n",
      "batch id: 219, average_total_r_per_ep: -4.53, time spent per 10 batches: 15.9s\n",
      "batch id: 229, average_total_r_per_ep: -21.67, time spent per 10 batches: 15.9s\n",
      "batch id: 239, average_total_r_per_ep: 5.39, time spent per 10 batches: 15.9s\n",
      "batch id: 249, average_total_r_per_ep: -3.14, time spent per 10 batches: 16.6s\n",
      "batch id: 259, average_total_r_per_ep: 108.97, time spent per 10 batches: 16.8s\n",
      "batch id: 269, average_total_r_per_ep: 206.65, time spent per 10 batches: 16.7s\n",
      "batch id: 279, average_total_r_per_ep: 40.84, time spent per 10 batches: 16.4s\n",
      "batch id: 289, average_total_r_per_ep: 51.79, time spent per 10 batches: 15.9s\n",
      "batch id: 299, average_total_r_per_ep: -8.87, time spent per 10 batches: 16.4s\n",
      "batch id: 309, average_total_r_per_ep: -46.06, time spent per 10 batches: 16.8s\n",
      "batch id: 319, average_total_r_per_ep: 9.39, time spent per 10 batches: 16.4s\n",
      "batch id: 329, average_total_r_per_ep: -12.39, time spent per 10 batches: 15.9s\n",
      "batch id: 339, average_total_r_per_ep: 17.97, time spent per 10 batches: 16.1s\n",
      "batch id: 349, average_total_r_per_ep: 3.90, time spent per 10 batches: 16.1s\n",
      "batch id: 359, average_total_r_per_ep: 29.43, time spent per 10 batches: 16.0s\n",
      "batch id: 369, average_total_r_per_ep: -22.34, time spent per 10 batches: 16.9s\n",
      "batch id: 379, average_total_r_per_ep: -10.36, time spent per 10 batches: 16.0s\n",
      "batch id: 389, average_total_r_per_ep: 5.14, time spent per 10 batches: 16.2s\n",
      "batch id: 399, average_total_r_per_ep: 15.51, time spent per 10 batches: 19.6s\n",
      "batch id: 409, average_total_r_per_ep: -0.04, time spent per 10 batches: 16.0s\n",
      "batch id: 419, average_total_r_per_ep: 14.50, time spent per 10 batches: 16.1s\n",
      "batch id: 429, average_total_r_per_ep: -9.83, time spent per 10 batches: 17.3s\n",
      "batch id: 439, average_total_r_per_ep: -13.40, time spent per 10 batches: 16.0s\n",
      "batch id: 449, average_total_r_per_ep: 12.54, time spent per 10 batches: 16.1s\n",
      "batch id: 459, average_total_r_per_ep: 3.36, time spent per 10 batches: 15.7s\n",
      "batch id: 469, average_total_r_per_ep: 14.61, time spent per 10 batches: 16.2s\n",
      "batch id: 479, average_total_r_per_ep: 125.80, time spent per 10 batches: 16.3s\n",
      "batch id: 489, average_total_r_per_ep: 116.72, time spent per 10 batches: 16.7s\n",
      "batch id: 499, average_total_r_per_ep: 15.77, time spent per 10 batches: 17.0s\n",
      "batch id: 509, average_total_r_per_ep: 75.07, time spent per 10 batches: 16.7s\n",
      "batch id: 519, average_total_r_per_ep: 158.84, time spent per 10 batches: 16.7s\n",
      "batch id: 529, average_total_r_per_ep: 136.31, time spent per 10 batches: 16.6s\n",
      "batch id: 539, average_total_r_per_ep: 91.24, time spent per 10 batches: 16.8s\n",
      "batch id: 549, average_total_r_per_ep: -39.82, time spent per 10 batches: 16.7s\n",
      "batch id: 559, average_total_r_per_ep: 76.22, time spent per 10 batches: 16.7s\n",
      "batch id: 569, average_total_r_per_ep: 124.96, time spent per 10 batches: 16.8s\n",
      "batch id: 579, average_total_r_per_ep: 91.75, time spent per 10 batches: 16.9s\n",
      "batch id: 589, average_total_r_per_ep: -201.76, time spent per 10 batches: 16.7s\n",
      "batch id: 599, average_total_r_per_ep: -58.37, time spent per 10 batches: 17.2s\n",
      "batch id: 609, average_total_r_per_ep: -10.23, time spent per 10 batches: 16.7s\n",
      "batch id: 619, average_total_r_per_ep: 83.77, time spent per 10 batches: 16.6s\n",
      "batch id: 629, average_total_r_per_ep: 121.47, time spent per 10 batches: 16.8s\n",
      "batch id: 639, average_total_r_per_ep: 185.85, time spent per 10 batches: 16.8s\n",
      "batch id: 649, average_total_r_per_ep: 220.23, time spent per 10 batches: 16.8s\n",
      "batch id: 659, average_total_r_per_ep: 72.51, time spent per 10 batches: 17.1s\n",
      "batch id: 669, average_total_r_per_ep: 313.01, time spent per 10 batches: 16.8s\n",
      "batch id: 679, average_total_r_per_ep: -153.15, time spent per 10 batches: 16.8s\n",
      "batch id: 689, average_total_r_per_ep: 102.02, time spent per 10 batches: 16.7s\n",
      "batch id: 699, average_total_r_per_ep: -184.84, time spent per 10 batches: 16.8s\n",
      "batch id: 709, average_total_r_per_ep: 162.84, time spent per 10 batches: 17.1s\n",
      "batch id: 719, average_total_r_per_ep: -62.50, time spent per 10 batches: 17.0s\n",
      "batch id: 729, average_total_r_per_ep: 62.43, time spent per 10 batches: 17.2s\n",
      "batch id: 739, average_total_r_per_ep: 223.73, time spent per 10 batches: 17.0s\n",
      "batch id: 749, average_total_r_per_ep: 98.97, time spent per 10 batches: 16.6s\n",
      "batch id: 759, average_total_r_per_ep: 70.24, time spent per 10 batches: 17.0s\n",
      "batch id: 769, average_total_r_per_ep: 120.90, time spent per 10 batches: 16.7s\n",
      "batch id: 779, average_total_r_per_ep: -4.61, time spent per 10 batches: 17.0s\n",
      "batch id: 789, average_total_r_per_ep: 158.14, time spent per 10 batches: 16.7s\n",
      "batch id: 799, average_total_r_per_ep: -10.50, time spent per 10 batches: 16.6s\n",
      "batch id: 809, average_total_r_per_ep: 122.47, time spent per 10 batches: 16.8s\n",
      "batch id: 819, average_total_r_per_ep: 77.99, time spent per 10 batches: 17.0s\n",
      "batch id: 829, average_total_r_per_ep: 9.45, time spent per 10 batches: 16.7s\n",
      "batch id: 839, average_total_r_per_ep: 12.37, time spent per 10 batches: 17.0s\n",
      "batch id: 849, average_total_r_per_ep: 232.80, time spent per 10 batches: 16.9s\n",
      "batch id: 859, average_total_r_per_ep: 125.15, time spent per 10 batches: 16.7s\n",
      "batch id: 869, average_total_r_per_ep: 216.71, time spent per 10 batches: 16.7s\n",
      "batch id: 879, average_total_r_per_ep: -17.91, time spent per 10 batches: 17.0s\n",
      "batch id: 889, average_total_r_per_ep: 36.69, time spent per 10 batches: 16.9s\n",
      "batch id: 899, average_total_r_per_ep: 166.72, time spent per 10 batches: 16.9s\n",
      "batch id: 909, average_total_r_per_ep: -82.38, time spent per 10 batches: 16.8s\n",
      "batch id: 919, average_total_r_per_ep: 315.67, time spent per 10 batches: 16.9s\n",
      "batch id: 929, average_total_r_per_ep: 126.21, time spent per 10 batches: 16.8s\n",
      "batch id: 939, average_total_r_per_ep: 285.35, time spent per 10 batches: 17.0s\n",
      "batch id: 949, average_total_r_per_ep: 64.19, time spent per 10 batches: 16.7s\n",
      "batch id: 959, average_total_r_per_ep: 29.52, time spent per 10 batches: 16.9s\n",
      "batch id: 969, average_total_r_per_ep: 35.79, time spent per 10 batches: 16.7s\n",
      "batch id: 979, average_total_r_per_ep: 146.69, time spent per 10 batches: 16.9s\n",
      "batch id: 989, average_total_r_per_ep: 192.17, time spent per 10 batches: 16.9s\n",
      "batch id: 999, average_total_r_per_ep: -128.20, time spent per 10 batches: 16.9s\n",
      "batch id: 1009, average_total_r_per_ep: 52.22, time spent per 10 batches: 16.9s\n",
      "batch id: 1019, average_total_r_per_ep: -22.53, time spent per 10 batches: 17.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch id: 1029, average_total_r_per_ep: 135.31, time spent per 10 batches: 16.7s\n",
      "batch id: 1039, average_total_r_per_ep: 73.97, time spent per 10 batches: 17.0s\n",
      "batch id: 1049, average_total_r_per_ep: 162.85, time spent per 10 batches: 16.7s\n",
      "batch id: 1059, average_total_r_per_ep: 157.87, time spent per 10 batches: 16.8s\n",
      "batch id: 1069, average_total_r_per_ep: 69.44, time spent per 10 batches: 16.8s\n",
      "batch id: 1079, average_total_r_per_ep: 236.68, time spent per 10 batches: 16.6s\n",
      "batch id: 1089, average_total_r_per_ep: 67.42, time spent per 10 batches: 17.0s\n",
      "batch id: 1099, average_total_r_per_ep: -8.56, time spent per 10 batches: 16.9s\n",
      "batch id: 1109, average_total_r_per_ep: 25.31, time spent per 10 batches: 16.7s\n",
      "batch id: 1119, average_total_r_per_ep: -32.08, time spent per 10 batches: 16.7s\n",
      "batch id: 1129, average_total_r_per_ep: 107.94, time spent per 10 batches: 16.6s\n",
      "batch id: 1139, average_total_r_per_ep: 89.03, time spent per 10 batches: 16.7s\n",
      "batch id: 1149, average_total_r_per_ep: 156.08, time spent per 10 batches: 16.6s\n",
      "batch id: 1159, average_total_r_per_ep: 121.53, time spent per 10 batches: 16.7s\n",
      "batch id: 1169, average_total_r_per_ep: -108.80, time spent per 10 batches: 16.6s\n",
      "batch id: 1179, average_total_r_per_ep: 46.69, time spent per 10 batches: 16.8s\n",
      "batch id: 1189, average_total_r_per_ep: 149.71, time spent per 10 batches: 16.5s\n",
      "batch id: 1199, average_total_r_per_ep: 253.89, time spent per 10 batches: 16.8s\n",
      "batch id: 1209, average_total_r_per_ep: -46.29, time spent per 10 batches: 16.7s\n",
      "batch id: 1219, average_total_r_per_ep: 27.64, time spent per 10 batches: 16.8s\n",
      "batch id: 1229, average_total_r_per_ep: 103.02, time spent per 10 batches: 17.0s\n",
      "batch id: 1239, average_total_r_per_ep: -94.00, time spent per 10 batches: 16.7s\n",
      "batch id: 1249, average_total_r_per_ep: 65.83, time spent per 10 batches: 16.9s\n",
      "batch id: 1259, average_total_r_per_ep: -55.47, time spent per 10 batches: 16.7s\n",
      "batch id: 1269, average_total_r_per_ep: 163.14, time spent per 10 batches: 16.9s\n",
      "batch id: 1279, average_total_r_per_ep: 70.81, time spent per 10 batches: 16.7s\n",
      "batch id: 1289, average_total_r_per_ep: 211.64, time spent per 10 batches: 16.9s\n",
      "batch id: 1299, average_total_r_per_ep: -7.71, time spent per 10 batches: 16.8s\n",
      "batch id: 1309, average_total_r_per_ep: 101.19, time spent per 10 batches: 17.0s\n",
      "batch id: 1319, average_total_r_per_ep: 154.18, time spent per 10 batches: 16.8s\n",
      "batch id: 1329, average_total_r_per_ep: 99.03, time spent per 10 batches: 17.1s\n",
      "batch id: 1339, average_total_r_per_ep: 96.64, time spent per 10 batches: 16.9s\n",
      "batch id: 1349, average_total_r_per_ep: 108.08, time spent per 10 batches: 16.7s\n",
      "batch id: 1359, average_total_r_per_ep: 137.72, time spent per 10 batches: 17.0s\n",
      "batch id: 1369, average_total_r_per_ep: 93.99, time spent per 10 batches: 16.5s\n",
      "batch id: 1379, average_total_r_per_ep: 108.52, time spent per 10 batches: 16.8s\n",
      "batch id: 1389, average_total_r_per_ep: -143.98, time spent per 10 batches: 16.8s\n",
      "batch id: 1399, average_total_r_per_ep: -25.95, time spent per 10 batches: 16.8s\n",
      "batch id: 1409, average_total_r_per_ep: -70.47, time spent per 10 batches: 16.5s\n",
      "batch id: 1419, average_total_r_per_ep: 82.47, time spent per 10 batches: 16.7s\n",
      "batch id: 1429, average_total_r_per_ep: 155.06, time spent per 10 batches: 16.9s\n",
      "batch id: 1439, average_total_r_per_ep: 45.60, time spent per 10 batches: 16.7s\n",
      "batch id: 1449, average_total_r_per_ep: 75.80, time spent per 10 batches: 16.8s\n",
      "batch id: 1459, average_total_r_per_ep: 3.82, time spent per 10 batches: 16.9s\n",
      "batch id: 1469, average_total_r_per_ep: -34.90, time spent per 10 batches: 16.7s\n",
      "batch id: 1479, average_total_r_per_ep: 215.48, time spent per 10 batches: 16.6s\n",
      "batch id: 1489, average_total_r_per_ep: 76.99, time spent per 10 batches: 16.8s\n",
      "batch id: 1499, average_total_r_per_ep: 141.20, time spent per 10 batches: 16.7s\n",
      "batch id: 1509, average_total_r_per_ep: 153.67, time spent per 10 batches: 16.9s\n",
      "batch id: 1519, average_total_r_per_ep: 172.48, time spent per 10 batches: 16.8s\n",
      "batch id: 1529, average_total_r_per_ep: -94.57, time spent per 10 batches: 16.8s\n",
      "batch id: 1539, average_total_r_per_ep: -66.07, time spent per 10 batches: 16.8s\n",
      "batch id: 1549, average_total_r_per_ep: 21.47, time spent per 10 batches: 16.7s\n",
      "batch id: 1559, average_total_r_per_ep: 252.41, time spent per 10 batches: 16.8s\n",
      "batch id: 1569, average_total_r_per_ep: -112.65, time spent per 10 batches: 16.9s\n",
      "batch id: 1579, average_total_r_per_ep: 129.52, time spent per 10 batches: 16.7s\n",
      "batch id: 1589, average_total_r_per_ep: 184.05, time spent per 10 batches: 16.8s\n",
      "batch id: 1599, average_total_r_per_ep: 216.39, time spent per 10 batches: 16.7s\n",
      "batch id: 1609, average_total_r_per_ep: 135.32, time spent per 10 batches: 17.1s\n",
      "batch id: 1619, average_total_r_per_ep: 202.01, time spent per 10 batches: 16.8s\n",
      "batch id: 1629, average_total_r_per_ep: -1.85, time spent per 10 batches: 17.0s\n",
      "batch id: 1639, average_total_r_per_ep: 311.06, time spent per 10 batches: 16.6s\n",
      "batch id: 1649, average_total_r_per_ep: 189.94, time spent per 10 batches: 16.7s\n",
      "batch id: 1659, average_total_r_per_ep: 58.48, time spent per 10 batches: 16.9s\n",
      "batch id: 1669, average_total_r_per_ep: 173.03, time spent per 10 batches: 16.9s\n",
      "batch id: 1679, average_total_r_per_ep: 88.28, time spent per 10 batches: 16.7s\n",
      "batch id: 1689, average_total_r_per_ep: -83.85, time spent per 10 batches: 16.6s\n",
      "batch id: 1699, average_total_r_per_ep: -39.55, time spent per 10 batches: 17.0s\n",
      "batch id: 1709, average_total_r_per_ep: 17.01, time spent per 10 batches: 16.6s\n",
      "batch id: 1719, average_total_r_per_ep: 159.76, time spent per 10 batches: 16.9s\n",
      "batch id: 1729, average_total_r_per_ep: -1.66, time spent per 10 batches: 16.9s\n",
      "batch id: 1739, average_total_r_per_ep: -53.86, time spent per 10 batches: 16.5s\n",
      "batch id: 1749, average_total_r_per_ep: 99.37, time spent per 10 batches: 16.7s\n",
      "batch id: 1759, average_total_r_per_ep: -9.69, time spent per 10 batches: 17.0s\n",
      "batch id: 1769, average_total_r_per_ep: 130.18, time spent per 10 batches: 17.0s\n",
      "batch id: 1779, average_total_r_per_ep: 14.77, time spent per 10 batches: 16.7s\n",
      "batch id: 1789, average_total_r_per_ep: 111.93, time spent per 10 batches: 16.8s\n",
      "batch id: 1799, average_total_r_per_ep: 47.21, time spent per 10 batches: 16.7s\n",
      "batch id: 1809, average_total_r_per_ep: 218.24, time spent per 10 batches: 16.6s\n",
      "batch id: 1819, average_total_r_per_ep: -30.15, time spent per 10 batches: 16.8s\n",
      "batch id: 1829, average_total_r_per_ep: 161.37, time spent per 10 batches: 17.0s\n",
      "batch id: 1839, average_total_r_per_ep: 154.82, time spent per 10 batches: 16.9s\n",
      "batch id: 1849, average_total_r_per_ep: 57.69, time spent per 10 batches: 17.0s\n",
      "batch id: 1859, average_total_r_per_ep: -116.32, time spent per 10 batches: 16.6s\n",
      "batch id: 1869, average_total_r_per_ep: 45.94, time spent per 10 batches: 16.8s\n",
      "batch id: 1879, average_total_r_per_ep: 147.41, time spent per 10 batches: 16.7s\n",
      "batch id: 1889, average_total_r_per_ep: 174.01, time spent per 10 batches: 16.7s\n",
      "batch id: 1899, average_total_r_per_ep: 254.09, time spent per 10 batches: 16.8s\n",
      "batch id: 1909, average_total_r_per_ep: 133.74, time spent per 10 batches: 16.7s\n",
      "batch id: 1919, average_total_r_per_ep: 105.51, time spent per 10 batches: 16.6s\n",
      "batch id: 1929, average_total_r_per_ep: -17.32, time spent per 10 batches: 16.6s\n",
      "batch id: 1939, average_total_r_per_ep: -24.64, time spent per 10 batches: 17.1s\n",
      "batch id: 1949, average_total_r_per_ep: 40.19, time spent per 10 batches: 16.7s\n",
      "Finished training~\n"
     ]
    }
   ],
   "source": [
    "# print parameters\n",
    "print('num_of_pair =', num_of_pair)\n",
    "print('batch_size =', batch_size)\n",
    "print('num_of_batch = {}, estimated epoch = {}'.format(num_of_batch, num_of_batch*batch_size/2/num_of_pair))\n",
    "print('rand_action_prob =', rand_action_prob)\n",
    "print('lr =', lr)\n",
    "\n",
    "# for training reference only\n",
    "average_total_r = np.zeros(batch_size)\n",
    "\n",
    "start_time = time.time()\n",
    "for batch in range(num_of_batch):\n",
    "    \n",
    "    # saving for update\n",
    "    all_logits = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    with tf.GradientTape() as gt:\n",
    "        done = False\n",
    "        s = env.reset(training=True)\n",
    "\n",
    "        # internally the episode length is fixed by trading_period\n",
    "        while not done:\n",
    "            logits = pi(s)\n",
    "            a = sample_action(logits, random=np.random.rand() <= rand_action_prob)\n",
    "            r, next_s, done = env.step(a.numpy())\n",
    "\n",
    "            # save the episode\n",
    "            all_logits.append(logits)\n",
    "            all_actions.append(a)\n",
    "            all_rewards.append(r)\n",
    "            \n",
    "            average_total_r += r\n",
    "            \n",
    "            # debugging\n",
    "#             print(env.t)\n",
    "#             print(env.t+1==200)\n",
    "#             print(r[0])\n",
    "#             print(env.total_portfolio_value[0])\n",
    "#             print(done)\n",
    "#             print(logits)\n",
    "\n",
    "        all_logits_stack = tf.stack(all_logits)\n",
    "        all_actions_stack = tf.stack(all_actions)\n",
    "        all_rewards_stack = np.array(all_rewards)\n",
    "        \n",
    "        # compute cummulative rewards for each action\n",
    "        all_cum_rewards = discount_rewards(all_rewards_stack)\n",
    "        all_cum_rewards -= np.mean(all_cum_rewards)\n",
    "        all_cum_rewards /= np.std(all_cum_rewards)\n",
    "#         all_cum_rewards /= np.abs(all_cum_rewards).max()\n",
    "        all_cum_rewards = tf.convert_to_tensor(all_cum_rewards, dtype=tf.float32)\n",
    "\n",
    "        loss_value = loss(all_logits_stack, all_actions_stack, all_cum_rewards)\n",
    "    \n",
    "    grads = gt.gradient(loss_value, state_encoding_model.variables + pi.variables)\n",
    "    optimizer.apply_gradients(zip(grads, state_encoding_model.variables + pi.variables))\n",
    "    \n",
    "    if (batch+1) % batch_per_print == 0:\n",
    "        end_time = time.time()\n",
    "        print(\"batch id: {}, average_total_r_per_ep: {:.2f}, time spent per {} batches: {:.1f}s\".format(\n",
    "            batch, np.mean(average_total_r/batch_per_print), batch_per_print, end_time-start_time))\n",
    "        average_total_r = np.zeros(batch_size)\n",
    "        start_time = time.time()\n",
    "print('Finished training~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test time\n",
    "total_r = np.zeros(batch_size)\n",
    "\n",
    "num_it = num_of_pair*2//batch_size\n",
    "\n",
    "rs = []\n",
    "\n",
    "for i in range(num_it):\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    # use non-training data to test\n",
    "    s = env.reset(training=False)\n",
    "\n",
    "    # internally the episode length is fixed by trading_period\n",
    "    while not done:\n",
    "        logits = pi(s)\n",
    "        a = sample_action(logits)\n",
    "        r, next_s, done = env.step(a.numpy())\n",
    "\n",
    "    #     print(r[0])\n",
    "    #     print(env.total_portfolio_value[0])\n",
    "    #     print(done)\n",
    "\n",
    "        total_r += r\n",
    "    rs.append(total_r)\n",
    "\n",
    "rs = np.array(rs).reshape(-1)\n",
    "# print(\"At test time, average_total_r_per_ep: {:.2f}\".format(np.mean(average_total_r/num_it)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 21.,  56.,  49., 126., 161., 189., 168.,  70.,  28.,  28.]),\n",
       " array([-13138.39646521, -10680.60028082,  -8222.80409643,  -5765.00791204,\n",
       "         -3307.21172765,   -849.41554326,   1608.38064113,   4066.17682552,\n",
       "          6523.97300991,   8981.7691943 ,  11439.56537869]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGvd/vw6NMOrcF/IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x153d28754cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
