{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext\n",
    "import random\n",
    "\n",
    "from process_raw_prices import *\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update batch size\n",
    "batch_size = 25\n",
    "\n",
    "# number of batch in training\n",
    "num_of_batch = 1000\n",
    "\n",
    "# fixed number of time steps in one episode (not used)\n",
    "trading_period = 1000\n",
    "\n",
    "# 1 is zscore\n",
    "num_features = 1\n",
    "\n",
    "# 0 is no position. 1 is long the spread. 2 is short the spread.\n",
    "a_num = position_num = 3\n",
    "\n",
    "# RNN hidden state dimension\n",
    "h_dim = 30\n",
    "\n",
    "# number of RNN layer\n",
    "num_layers = 1\n",
    "\n",
    "# number of layer1 output\n",
    "layer1_out_num = 30\n",
    "\n",
    "# learning rate\n",
    "lr = 5e-3\n",
    "\n",
    "# discount factor in reinforcement learning\n",
    "gamma = 1\n",
    "\n",
    "# random action probability\n",
    "rand_action_prob = 0.1\n",
    "\n",
    "batch_per_print = 1\n",
    "\n",
    "# dummy initial cash\n",
    "initial_cash = 10000\n",
    "\n",
    "# processed dataset folder path\n",
    "dataset_folder_path = '../../dataset/nyse-daily-transformed'\n",
    "os.makedirs(dataset_folder_path, exist_ok=True)\n",
    "\n",
    "# raw dataset files pattern\n",
    "raw_files_path_pattern = \"../../dataset/nyse-daily/*.csv\"\n",
    "\n",
    "df_columns = ['close1', 'close2', 'spread', 'logClose1', 'logClose2', 'zscore']\n",
    "ind = {'y_close': 0, 'x_close': 1, 'spread': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute dataset for training\n",
    "all_pairs_slices = [splitext(f)[0] for f in listdir(dataset_folder_path) if isfile(join(dataset_folder_path, f))]\n",
    "if len(all_pairs_slices) == 0:\n",
    "    generate_pairs_training_data(raw_files_path_pattern=raw_files_path_pattern,\n",
    "                                 result_path=dataset_folder_path,\n",
    "                                 min_size=252*4,\n",
    "                                 training_period=52,\n",
    "                                 points_per_cut=252\n",
    "                                )\n",
    "    all_pairs_slices = [splitext(f)[0] for f in listdir(dataset_folder_path) if isfile(join(dataset_folder_path, f))]\n",
    "print(\"Total number of pair slices: %d\" % len(all_pairs_slices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_random_history(batch_size):\n",
    "    \"\"\"Sample some pairs and get the history of those pairs. The history should have\n",
    "    three dimension. The first dimension is for time. The second dimension is indexed\n",
    "    by features name. The third dimension is the index of training instance.\n",
    "    \"\"\"\n",
    "    sample_pair_slices = random.sample(all_pairs_slices, batch_size)\n",
    "    history = []\n",
    "    for s in sample_pair_slices:\n",
    "        df = pd.read_csv(join(dataset_folder_path, s+\".csv\"))\n",
    "        df_val = df[df_columns].values\n",
    "        history.append(df_val)\n",
    "    \n",
    "    history = np.array(history)\n",
    "    return np.transpose(history, (1, 2, 0))\n",
    "\n",
    "def compute_input_history(history):\n",
    "    \"\"\"Slicing history in its second dimension.\"\"\"\n",
    "    # no slicing for now\n",
    "    return history\n",
    "\n",
    "def sample_action(logits, random=False):\n",
    "    if random:\n",
    "        dist = tf.distributions.Categorical(logits=tf.zeros([batch_size, a_num]))\n",
    "    else:\n",
    "        dist = tf.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    # 1-D Tensor where the i-th element correspond to a sample from\n",
    "    # the i-th categorical distribution\n",
    "    return dist.sample()\n",
    "\n",
    "def long_portfolio_value(q, p):\n",
    "    return q*p\n",
    "\n",
    "def short_portfolio_value(q, p, init_p):\n",
    "    return q*(3.0*init_p/2 - p)\n",
    "\n",
    "# def discount_rewards(r, all_actions):\n",
    "#     \"\"\"\n",
    "#     r is a numpy array in the shape of (n, batch_size).\n",
    "#     all_actions is a numpy array in the same shape as r.\n",
    "    \n",
    "#     return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "#     result = np.zeros_like(r, dtype=float)\n",
    "#     n = r.shape[0]\n",
    "#     sum_ = np.zeros_like(r[0], dtype=float)\n",
    "#     pre_action = all_actions[n-1]\n",
    "#     for i in range(n-1,-1,-1):\n",
    "#         sum_ *= gamma\n",
    "        \n",
    "#         # when the previous action(position) not equal to the current one,\n",
    "#         # set the previous sum of reward to be zero.\n",
    "#         sum_ = sum_*(all_actions[i]==pre_action) + r[i]\n",
    "#         result[i] = sum_\n",
    "        \n",
    "#         # update pre_action\n",
    "#         pre_action = all_actions[i]\n",
    "    \n",
    "#     return result\n",
    "\n",
    "def discount_rewards(r, all_actions):\n",
    "    \"\"\"\n",
    "    r is a numpy array in the shape of (n, batch_size).\n",
    "    all_actions is a numpy array in the same shape as r.\n",
    "    \n",
    "    return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "    result = np.zeros_like(r, dtype=float)\n",
    "    n = r.shape[0]\n",
    "    sum_ = np.zeros_like(r[0], dtype=float)\n",
    "    pre_action = all_actions[n-1]\n",
    "    for i in range(n-1,-1,-1):\n",
    "        sum_ *= gamma\n",
    "        sum_ += r[i]\n",
    "        result[i] = sum_\n",
    "    \n",
    "    return result\n",
    "\n",
    "def loss(all_logits, all_actions, all_advantages):\n",
    "    neg_log_select_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_logits, labels=all_actions)\n",
    "    \n",
    "    # 0 axis is the time axis. 1 axis is the batch axis\n",
    "    return tf.reduce_mean(neg_log_select_prob * all_advantages, 0)\n",
    "\n",
    "# classes\n",
    "class TradingPolicyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TradingPolicyModel, self).__init__()\n",
    "        self.dense1 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense2 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense3 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense4 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.logits = tf.layers.Dense(units=a_num,\n",
    "                                      activation=tf.keras.layers.LeakyReLU(),\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        inputs = self.dense1(inputs)\n",
    "        inputs = self.dense2(inputs)\n",
    "        inputs = self.dense3(inputs)\n",
    "        inputs = self.dense4(inputs)\n",
    "        logits = self.logits(inputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class StateEncodingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(StateEncodingModel, self).__init__()\n",
    "        self.cell_layer = tf.contrib.rnn.LSTMCell(h_dim)\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell([self.cell_layer] * num_layers)\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        oberservation, self.state = self.cell(inputs, self.state)\n",
    "        return oberservation\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "class TradingEnvironment():\n",
    "    \"\"\"Trading environment for reinforcement learning training.\n",
    "    \n",
    "    Arguments:\n",
    "        state_encoding_model: the model that encode past input_history data into a state\n",
    "        vector which will be fed as input to the policy network.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_encoding_model):\n",
    "        # do some initialization\n",
    "        self.state_encoding_model = state_encoding_model\n",
    "        self._reset_env()\n",
    "        \n",
    "    def _reset_env(self):\n",
    "        self.t = 0\n",
    "        self.state_encoding_model.reset_state()\n",
    "\n",
    "        # 0 is no position. 1 is long the spread. 2 is short the spread\n",
    "        self.position = np.zeros(batch_size, dtype=int)\n",
    "        \n",
    "        # initialize the cash each agent has\n",
    "        self.total_portfolio_value = np.ones(batch_size)*initial_cash\n",
    "        \n",
    "        # only useful when there is a postion on the spread\n",
    "        self.quantity = {'x': np.zeros(batch_size), 'y': np.zeros(batch_size)}\n",
    "        \n",
    "        # for compute current portfolio value of the short side\n",
    "        self.short_side_init_price = np.zeros(batch_size)\n",
    "\n",
    "        # prepare a batch of history and input_history\n",
    "        self.history = get_random_history(batch_size)\n",
    "        self.input_history = compute_input_history(self.history)\n",
    "        \n",
    "        # create or update self.state variable\n",
    "        self.update_state()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Return an initial state for the trading environment\"\"\"\n",
    "        if self.t == 0:\n",
    "            return self.state\n",
    "        else:\n",
    "            self._reset_env()\n",
    "            return self.state\n",
    "    \n",
    "    def compute_reward(self, action):\n",
    "        \"\"\"Compute the reward at time t which is the change in total portfolio value\n",
    "        from time t to t+1. It also update the position for time t+1. Exit trade when\n",
    "        the short side portfolio value <= 0.\"\"\"\n",
    "        \n",
    "        r = np.zeros_like(action, dtype=float)\n",
    "        cur_his = self.history[self.t]\n",
    "        nex_his = self.history[self.t+1]\n",
    "        \n",
    "        # compute for each training instance in a batch\n",
    "        for i, a in enumerate(action):\n",
    "            y_p = cur_his[ind[\"y_close\"], i]\n",
    "            x_p = cur_his[ind[\"x_close\"], i]\n",
    "            nex_y_p = nex_his[ind[\"y_close\"], i]\n",
    "            nex_x_p = nex_his[ind[\"x_close\"], i]\n",
    "            \n",
    "            \n",
    "            if a == 0: # take no position on the spread\n",
    "                # no change in portfolio value\n",
    "                r[i] = 0\n",
    "                self.position[i] = 0\n",
    "                self.quantity['y'][i] = 0.0\n",
    "                self.quantity['x'][i] = 0.0\n",
    "            elif a == 1: # long the spread: long Y and short X\n",
    "                # quantity of each stock will change when the current position is not previous position\n",
    "                if self.position[i] == 0 or self.position[i] == 2:\n",
    "                    # compute quantity from cash\n",
    "                    self.quantity['y'][i] = 2.0*self.total_portfolio_value[i]/3.0/y_p\n",
    "                    self.quantity['x'][i] = 2.0*self.total_portfolio_value[i]/3.0/x_p\n",
    "                    self.short_side_init_price[i] = x_p\n",
    "\n",
    "                lpv = long_portfolio_value(self.quantity['y'][i], nex_y_p)\n",
    "                spv = short_portfolio_value(self.quantity['x'][i], nex_x_p, self.short_side_init_price[i])\n",
    "                \n",
    "                # the zero here can be changed to other positive threshold ...\n",
    "                if spv <= 0:\n",
    "                    # we loss all the money in the short side\n",
    "                    nex_portfolio_value = lpv\n",
    "\n",
    "                    # forced to take position 0\n",
    "                    self.postion[i] = 0\n",
    "                else:\n",
    "                    nex_portfolio_value = lpv + spv\n",
    "                    self.position[i] = 1\n",
    "                \n",
    "                r[i] = nex_portfolio_value - self.total_portfolio_value[i]\n",
    "                self.total_portfolio_value[i] = nex_portfolio_value\n",
    "            elif a == 2: # short the spread: short Y and long X\n",
    "                # quantity will change when the current position is not previous position\n",
    "                if self.position[i] == 0 or self.position[i] == 1:\n",
    "                    # compute quantity from cash\n",
    "                    self.quantity['y'][i] = 2.0*self.total_portfolio_value[i]/3.0/y_p\n",
    "                    self.quantity['x'][i] = 2.0*self.total_portfolio_value[i]/3.0/x_p\n",
    "                    self.short_side_init_price[i] = y_p\n",
    "\n",
    "                lpv = long_portfolio_value(self.quantity['x'][i], nex_x_p)\n",
    "                spv = short_portfolio_value(self.quantity['y'][i], nex_y_p, self.short_side_init_price[i])\n",
    "                \n",
    "                if spv <= 0:\n",
    "                    # we loss all the money in the short side\n",
    "                    nex_portfolio_value = lpv\n",
    "\n",
    "                    # forced to take position 0\n",
    "                    self.postion[i] = 0\n",
    "                else:\n",
    "                    nex_portfolio_value = lpv + spv\n",
    "                    self.position[i] = 2\n",
    "                \n",
    "                r[i] = nex_portfolio_value - self.total_portfolio_value[i]\n",
    "                self.total_portfolio_value[i] = nex_portfolio_value\n",
    "        return r\n",
    "    \n",
    "    def update_state(self):\n",
    "#         # concate next_input_history and next position to form next partial state\n",
    "#         partial_state = tf.concat([self.input_history[self.t].T, tf.one_hot(self.position, position_num)], 1)\n",
    "        \n",
    "#         # update state\n",
    "#         self.state = self.state_encoding_model(partial_state)\n",
    "\n",
    "        partial_state = self.input_history[self.t].T\n",
    "        self.state = tf.concat([\n",
    "            partial_state,\n",
    "            self.total_portfolio_value.T,\n",
    "            self.quantity['y'].T,\n",
    "            self.quantity['x'].T,\n",
    "            tf.one_hot(self.position, position_num)\n",
    "        ], 1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Given the current state and action, return the reward, next state and done.\n",
    "        This function should be called after reset.\n",
    "        \n",
    "        reward is of type numpy array. state is of type tensor. done is of type boolean.\n",
    "        \n",
    "        \n",
    "        Arguments:\n",
    "            action: a numpy array containing the current action for each training pair.\n",
    "\n",
    "        Note that we follow the convention where the trajectory is indexed as s_0, a_0, r_0,\n",
    "        s_1, ... . Therefore t is updated just after computing the reward is computed and\n",
    "        before computing next state.\n",
    "        \"\"\"\n",
    "        # r_t\n",
    "        r = self.compute_reward(action) # also update the position for time t+1\n",
    "\n",
    "        # t = t+1\n",
    "        self.t += 1\n",
    "        \n",
    "        # compute s_(t+1)\n",
    "        self.update_state()\n",
    "\n",
    "        return r, self.state, self.t+1 == trading_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = TradingPolicyModel()\n",
    "state_encoding_model = StateEncodingModel()\n",
    "env = TradingEnvironment(state_encoding_model)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "# for training reference only\n",
    "average_total_r = np.zeros(batch_size)\n",
    "\n",
    "for batch in range(num_of_batch):\n",
    "    \n",
    "    # saving for update\n",
    "    all_logits = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    with tf.GradientTape() as gt:\n",
    "        done = False\n",
    "        s = env.reset()\n",
    "\n",
    "        # internally the episode length is fixed by trading_period\n",
    "        while not done:\n",
    "            logits = pi(s)\n",
    "            a = sample_action(logits, random=np.random.rand() <= rand_action_prob)\n",
    "            r, next_s, done = env.step(a.numpy())\n",
    "\n",
    "            # save the episode\n",
    "            all_logits.append(logits)\n",
    "            all_actions.append(a)\n",
    "            all_rewards.append(r)\n",
    "            \n",
    "            average_total_r += r\n",
    "            \n",
    "#             # debugging\n",
    "#             print(r[0])\n",
    "#             print(env.total_portfolio_value[0])\n",
    "#             print(done)\n",
    "\n",
    "        all_logits_stack = tf.stack(all_logits)\n",
    "        all_actions_stack = tf.stack(all_actions)\n",
    "        all_rewards_stack = np.array(all_rewards)\n",
    "        \n",
    "        # compute cummulative rewards for each action\n",
    "        all_cum_rewards = discount_rewards(all_rewards_stack, all_actions_stack.numpy())\n",
    "#         all_cum_rewards -= np.mean(all_cum_rewards)\n",
    "#         all_cum_rewards /= np.std(all_cum_rewards)\n",
    "        all_cum_rewards = tf.convert_to_tensor(all_cum_rewards, dtype=tf.float32)\n",
    "\n",
    "        loss_value = loss(all_logits_stack, all_actions_stack, all_cum_rewards)\n",
    "    \n",
    "    if (batch+1) % batch_per_print == 0:\n",
    "        print(\"batch id: {}, average_total_r_per_ep: {}\".format(batch, np.mean(average_total_r)/batch_per_print))\n",
    "        average_total_r = np.zeros(batch_size)\n",
    "    \n",
    "    grads = gt.gradient(loss_value, state_encoding_model.variables + pi.variables)\n",
    "    optimizer.apply_gradients(zip(grads, state_encoding_model.variables + pi.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test time\n",
    "average_total_r = np.zeros(batch_size)\n",
    "done = False\n",
    "s = env.reset()\n",
    "\n",
    "# internally the episode length is fixed by trading_period\n",
    "while not done:\n",
    "    logits = pi(s)\n",
    "    a = sample_action(logits)\n",
    "    r, next_s, done = env.step(a.numpy())\n",
    "\n",
    "    # save the episode\n",
    "    all_logits.append(logits)\n",
    "    all_actions.append(a)\n",
    "    all_rewards.append(r)\n",
    "\n",
    "    average_total_r += r\n",
    "\n",
    "print(\"At test time, average_total_r_per_ep: {}\".format(np.mean(average_total_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
