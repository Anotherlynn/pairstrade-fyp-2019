{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from process_raw_prices import *\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update batch size\n",
    "batch_size = 2\n",
    "\n",
    "# number of batch in training\n",
    "num_of_batch = 500\n",
    "\n",
    "# fixed number of time steps in one episode (not used)\n",
    "trading_period = 200\n",
    "\n",
    "# 1 is zscore\n",
    "num_features = 1\n",
    "\n",
    "# 0 is no position. 1 is long the spread. 2 is short the spread.\n",
    "a_num = position_num = 3\n",
    "\n",
    "# RNN hidden state dimension\n",
    "h_dim = 30\n",
    "\n",
    "# number of RNN layer\n",
    "num_layers = 1\n",
    "\n",
    "# number of layer1 output\n",
    "layer1_out_num = 30\n",
    "\n",
    "# learning rate\n",
    "lr = 3e-3\n",
    "\n",
    "# discount factor in reinforcement learning\n",
    "gamma = 1\n",
    "\n",
    "# random action probability\n",
    "rand_action_prob = 0.05\n",
    "\n",
    "batch_per_print = 50\n",
    "\n",
    "# dummy initial cash\n",
    "initial_cash = 10000\n",
    "\n",
    "# processed dataset folder path\n",
    "dataset_folder_path = '../../dataset/nyse-daily-transformed'\n",
    "os.makedirs(dataset_folder_path, exist_ok=True)\n",
    "\n",
    "# raw dataset files pattern\n",
    "raw_files_path_pattern = \"../../dataset/nyse-daily/*.csv\"\n",
    "\n",
    "df_columns = ['close1', 'close2', 'spread', 'logClose1', 'logClose2', 'zscore']\n",
    "ind = {'y_close': 0, 'x_close': 1, 'spread': 2}\n",
    "\n",
    "# checkpoint folder\n",
    "checkpoint_dir = '../../model_checkpoint/'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pair slices for training: 2\n",
      "Total number of pair slices for testing: 2\n"
     ]
    }
   ],
   "source": [
    "# # compute dataset for training\n",
    "# all_pairs_slices = [splitext(f)[0] for f in listdir(dataset_folder_path) if isfile(join(dataset_folder_path, f))]\n",
    "# if len(all_pairs_slices) == 0:\n",
    "#     generate_pairs_training_data(raw_files_path_pattern=raw_files_path_pattern,\n",
    "#                                  result_path=dataset_folder_path,\n",
    "#                                  min_size=252*4,\n",
    "#                                  training_period=52,\n",
    "#                                  points_per_cut=252\n",
    "#                                 )\n",
    "#     all_pairs_slices = [splitext(f)[0] for f in listdir(dataset_folder_path) if isfile(join(dataset_folder_path, f))]\n",
    "# print(\"Total number of pair slices: %d\" % len(all_pairs_slices))\n",
    "\n",
    "# # split for training and testing\n",
    "# all_pairs = list(set(['-'.join(p.split('-')[0:2]) for p in all_pairs_slices]))[:2]\n",
    "# all_pairs = [\"VMW-WUBA\"]\n",
    "all_pairs = [\"TWTR-UIS\"]\n",
    "all_pairs_slices_train = []\n",
    "all_pairs_slices_test = []\n",
    "for p in all_pairs:\n",
    "    all_pairs_slices_train += [p+'-0', p+'-1']\n",
    "    all_pairs_slices_test += [p+'-2', p+'-3']\n",
    "print(\"Total number of pair slices for training: %d\" % len(all_pairs_slices_train))\n",
    "print(\"Total number of pair slices for testing: %d\" % len(all_pairs_slices_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_random_history(batch_size, training):\n",
    "    \"\"\"Sample some pairs and get the history of those pairs. The history should have\n",
    "    three dimension. The first dimension is for time. The second dimension is indexed\n",
    "    by features name. The third dimension is the index of training instance.\n",
    "    \"\"\"\n",
    "    sample_pair_slices = random.sample(all_pairs_slices_train if training else all_pairs_slices_test, batch_size)\n",
    "    history = []\n",
    "    for s in sample_pair_slices:\n",
    "        df = pd.read_csv(join(dataset_folder_path, s+\".csv\"))\n",
    "        df_val = df[df_columns].values\n",
    "        history.append(df_val)\n",
    "    \n",
    "    history = np.array(history)\n",
    "    return np.transpose(history, (1, 2, 0))\n",
    "\n",
    "def compute_input_history(history):\n",
    "    \"\"\"Slicing history in its second dimension.\"\"\"\n",
    "    # no slicing for now\n",
    "    return history\n",
    "\n",
    "def sample_action(logits, random=False):\n",
    "    if random:\n",
    "        dist = tf.distributions.Categorical(logits=tf.zeros([batch_size, a_num]))\n",
    "    else:\n",
    "        dist = tf.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    # 1-D Tensor where the i-th element correspond to a sample from\n",
    "    # the i-th categorical distribution\n",
    "    return dist.sample()\n",
    "\n",
    "def long_portfolio_value(q, p):\n",
    "    return q*p\n",
    "\n",
    "def short_portfolio_value(q, p, init_p):\n",
    "    return q*(3.0*init_p/2 - p)\n",
    "\n",
    "# def discount_rewards(r, all_actions):\n",
    "#     \"\"\"\n",
    "#     r is a numpy array in the shape of (n, batch_size).\n",
    "#     all_actions is a numpy array in the same shape as r.\n",
    "    \n",
    "#     return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "#     result = np.zeros_like(r, dtype=float)\n",
    "#     n = r.shape[0]\n",
    "#     sum_ = np.zeros_like(r[0], dtype=float)\n",
    "#     pre_action = all_actions[n-1]\n",
    "#     for i in range(n-1,-1,-1):\n",
    "#         sum_ *= gamma\n",
    "        \n",
    "#         # when the previous action(position) not equal to the current one,\n",
    "#         # set the previous sum of reward to be zero.\n",
    "#         sum_ = sum_*(all_actions[i]==pre_action) + r[i]\n",
    "#         result[i] = sum_\n",
    "        \n",
    "#         # update pre_action\n",
    "#         pre_action = all_actions[i]\n",
    "    \n",
    "#     return result\n",
    "\n",
    "def discount_rewards(r, all_actions):\n",
    "    \"\"\"\n",
    "    r is a numpy array in the shape of (n, batch_size).\n",
    "    all_actions is a numpy array in the same shape as r.\n",
    "    \n",
    "    return the discounted and cumulative rewards\"\"\"\n",
    "    \n",
    "    result = np.zeros_like(r, dtype=float)\n",
    "    n = r.shape[0]\n",
    "    sum_ = np.zeros_like(r[0], dtype=float)\n",
    "    pre_action = all_actions[n-1]\n",
    "    for i in range(n-1,-1,-1):\n",
    "        sum_ *= gamma\n",
    "        sum_ += r[i]\n",
    "        result[i] = sum_\n",
    "    \n",
    "    return result\n",
    "\n",
    "def loss(all_logits, all_actions, all_advantages):\n",
    "    neg_log_select_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=all_logits, labels=all_actions)\n",
    "    \n",
    "    # 0 axis is the time axis. 1 axis is the batch axis\n",
    "    return tf.reduce_mean(neg_log_select_prob * all_advantages, 0)\n",
    "\n",
    "def save_model():\n",
    "    hkg_time = datetime.now() + timedelta(hours=16)\n",
    "    checkpoint_name = hkg_time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, checkpoint_name)\n",
    "    root.save(checkpoint_prefix)\n",
    "    tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "def restore_model(checkpoint_name):\n",
    "    root.restore(join(checkpoint_dir, checkpoint_name))\n",
    "\n",
    "\n",
    "myLeakyReLU = tf.keras.layers.LeakyReLU()\n",
    "myLeakyReLU.__name__ = \"myLeakyReLU\"\n",
    "\n",
    "# classes\n",
    "class TradingPolicyModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TradingPolicyModel, self).__init__()\n",
    "        self.dense1 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense2 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "        self.dense3 = tf.layers.Dense(units=layer1_out_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "#         self.dense4 = tf.layers.Dense(units=layer1_out_num,\n",
    "#                                       activation=tf.keras.layers.LeakyReLU(),\n",
    "#                                       kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "#                                      )\n",
    "        self.logits = tf.layers.Dense(units=a_num,\n",
    "                                      activation=myLeakyReLU,\n",
    "                                      kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                                     )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        inputs = self.dense1(inputs)\n",
    "        inputs = self.dense2(inputs)\n",
    "        inputs = self.dense3(inputs)\n",
    "#         inputs = self.dense4(inputs)\n",
    "        logits = self.logits(inputs)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class StateEncodingModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(StateEncodingModel, self).__init__()\n",
    "        self.cell_layer = tf.contrib.rnn.LSTMCell(h_dim)\n",
    "        self.cell = tf.contrib.rnn.MultiRNNCell([self.cell_layer] * num_layers)\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        oberservation, self.state = self.cell(inputs, self.state)\n",
    "        return oberservation\n",
    "        \n",
    "    def reset_state(self):\n",
    "        self.state = self.cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "\n",
    "class TradingEnvironment():\n",
    "    \"\"\"Trading environment for reinforcement learning training.\n",
    "    \n",
    "    Arguments:\n",
    "        state_encoding_model: the model that encode past input_history data into a state\n",
    "        vector which will be fed as input to the policy network.\n",
    "    \"\"\"\n",
    "    def __init__(self, state_encoding_model):\n",
    "        # do some initialization\n",
    "        self.state_encoding_model = state_encoding_model\n",
    "        self._reset_env()\n",
    "        \n",
    "    def _reset_env(self, training=True):\n",
    "        self.t = 0\n",
    "        self.state_encoding_model.reset_state()\n",
    "\n",
    "        # 0 is no position. 1 is long the spread. 2 is short the spread\n",
    "        self.position = np.zeros(batch_size, dtype=int)\n",
    "        \n",
    "        # initialize the cash each agent has\n",
    "        self.total_portfolio_value = np.ones(batch_size)*initial_cash\n",
    "        \n",
    "        # only useful when there is a postion on the spread\n",
    "        self.quantity = {'x': np.zeros(batch_size), 'y': np.zeros(batch_size)}\n",
    "        \n",
    "        # for compute current portfolio value of the short side\n",
    "        self.short_side_init_price = np.zeros(batch_size)\n",
    "\n",
    "        # prepare a batch of history and input_history\n",
    "        self.history = get_random_history(batch_size, training)\n",
    "        self.input_history = compute_input_history(self.history)\n",
    "        \n",
    "        # create or update self.state variable\n",
    "        self.update_state()\n",
    "    \n",
    "    def reset(self, training=True):\n",
    "        \"\"\"Return an initial state for the trading environment\"\"\"\n",
    "        if self.t == 0 and True:\n",
    "            return self.state\n",
    "        else:\n",
    "            self._reset_env(training=training)\n",
    "            return self.state\n",
    "    \n",
    "    def compute_reward(self, action):\n",
    "        \"\"\"Compute the reward at time t which is the change in total portfolio value\n",
    "        from time t to t+1. It also update the position for time t+1. Exit trade when\n",
    "        the short side portfolio value <= 0.\"\"\"\n",
    "        \n",
    "        r = np.zeros_like(action, dtype=float)\n",
    "        cur_his = self.history[self.t]\n",
    "        nex_his = self.history[self.t+1]\n",
    "        \n",
    "        # compute for each training instance in a batch\n",
    "        for i, a in enumerate(action):\n",
    "            y_p = cur_his[ind[\"y_close\"], i]\n",
    "            x_p = cur_his[ind[\"x_close\"], i]\n",
    "            nex_y_p = nex_his[ind[\"y_close\"], i]\n",
    "            nex_x_p = nex_his[ind[\"x_close\"], i]\n",
    "            \n",
    "            \n",
    "            if a == 0: # take no position on the spread\n",
    "                # no change in portfolio value\n",
    "                r[i] = 0\n",
    "                self.position[i] = 0\n",
    "                self.quantity['y'][i] = 0.0\n",
    "                self.quantity['x'][i] = 0.0\n",
    "            elif a == 1: # long the spread: long Y and short X\n",
    "                # quantity of each stock will change when the current position is not previous position\n",
    "                if self.position[i] == 0 or self.position[i] == 2:\n",
    "                    # compute quantity from cash\n",
    "                    self.quantity['y'][i] = 2.0*self.total_portfolio_value[i]/3.0/y_p\n",
    "                    self.quantity['x'][i] = 2.0*self.total_portfolio_value[i]/3.0/x_p\n",
    "                    self.short_side_init_price[i] = x_p\n",
    "\n",
    "                lpv = long_portfolio_value(self.quantity['y'][i], nex_y_p)\n",
    "                spv = short_portfolio_value(self.quantity['x'][i], nex_x_p, self.short_side_init_price[i])\n",
    "                \n",
    "                # the zero here can be changed to other positive threshold ...\n",
    "                if spv <= 0:\n",
    "                    # we loss all the money in the short side\n",
    "                    nex_portfolio_value = lpv\n",
    "\n",
    "                    # forced to take position 0\n",
    "                    self.position[i] = 0\n",
    "                else:\n",
    "                    nex_portfolio_value = lpv + spv\n",
    "                    self.position[i] = 1\n",
    "                \n",
    "                r[i] = nex_portfolio_value - self.total_portfolio_value[i]\n",
    "                self.total_portfolio_value[i] = nex_portfolio_value\n",
    "            elif a == 2: # short the spread: short Y and long X\n",
    "                # quantity will change when the current position is not previous position\n",
    "                if self.position[i] == 0 or self.position[i] == 1:\n",
    "                    # compute quantity from cash\n",
    "                    self.quantity['y'][i] = 2.0*self.total_portfolio_value[i]/3.0/y_p\n",
    "                    self.quantity['x'][i] = 2.0*self.total_portfolio_value[i]/3.0/x_p\n",
    "                    self.short_side_init_price[i] = y_p\n",
    "\n",
    "                lpv = long_portfolio_value(self.quantity['x'][i], nex_x_p)\n",
    "                spv = short_portfolio_value(self.quantity['y'][i], nex_y_p, self.short_side_init_price[i])\n",
    "                \n",
    "                if spv <= 0:\n",
    "                    # we loss all the money in the short side\n",
    "                    nex_portfolio_value = lpv\n",
    "\n",
    "                    # forced to take position 0\n",
    "                    self.position[i] = 0\n",
    "                else:\n",
    "                    nex_portfolio_value = lpv + spv\n",
    "                    self.position[i] = 2\n",
    "                \n",
    "                r[i] = nex_portfolio_value - self.total_portfolio_value[i]\n",
    "                self.total_portfolio_value[i] = nex_portfolio_value\n",
    "        return r\n",
    "    \n",
    "    def update_state(self):\n",
    "#         # concate next_input_history and next position to form next partial state\n",
    "#         partial_state = tf.concat([self.input_history[self.t].T, tf.one_hot(self.position, position_num)], 1)\n",
    "        \n",
    "#         # update state\n",
    "#         self.state = self.state_encoding_model(partial_state)\n",
    "\n",
    "        partial_state = self.input_history[self.t].T\n",
    "        self.state = tf.concat([\n",
    "            partial_state,\n",
    "            np.array([self.total_portfolio_value,\n",
    "                      self.quantity['y'],\n",
    "                      self.quantity['x']]).T,\n",
    "            tf.one_hot(self.position, position_num)\n",
    "        ], 1)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Given the current state and action, return the reward, next state and done.\n",
    "        This function should be called after reset.\n",
    "        \n",
    "        reward is of type numpy array. state is of type tensor. done is of type boolean.\n",
    "        \n",
    "        \n",
    "        Arguments:\n",
    "            action: a numpy array containing the current action for each training pair.\n",
    "\n",
    "        Note that we follow the convention where the trajectory is indexed as s_0, a_0, r_0,\n",
    "        s_1, ... . Therefore t is updated just after computing the reward is computed and\n",
    "        before computing next state.\n",
    "        \"\"\"\n",
    "        # r_t\n",
    "        r = self.compute_reward(action) # also update the position for time t+1\n",
    "\n",
    "        # t = t+1\n",
    "        self.t += 1\n",
    "        \n",
    "        # compute s_(t+1)\n",
    "        self.update_state()\n",
    "\n",
    "        return r, self.state, (self.t+1) == trading_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create objects\n",
    "pi = TradingPolicyModel()\n",
    "state_encoding_model = StateEncodingModel()\n",
    "env = TradingEnvironment(state_encoding_model)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "# create checkpoint object\n",
    "root = tf.train.Checkpoint(pi=pi, state_encoding_model=state_encoding_model, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch id: 49, average_total_r_per_ep: -136.60943314153099\n",
      "batch id: 99, average_total_r_per_ep: 84.78978871643503\n",
      "batch id: 149, average_total_r_per_ep: 94.96325589298998\n",
      "batch id: 199, average_total_r_per_ep: 34.30157620589861\n",
      "batch id: 249, average_total_r_per_ep: -129.67120068977732\n",
      "batch id: 299, average_total_r_per_ep: 13745.563335705961\n",
      "batch id: 349, average_total_r_per_ep: 19227.10060887947\n",
      "batch id: 399, average_total_r_per_ep: 18229.183469552598\n",
      "batch id: 449, average_total_r_per_ep: 18583.84062468518\n",
      "batch id: 499, average_total_r_per_ep: 19358.416424155956\n"
     ]
    }
   ],
   "source": [
    "# for training reference only\n",
    "average_total_r = np.zeros(batch_size)\n",
    "\n",
    "for batch in range(num_of_batch):\n",
    "    \n",
    "    # saving for update\n",
    "    all_logits = []\n",
    "    all_actions = []\n",
    "    all_rewards = []\n",
    "    with tf.GradientTape() as gt:\n",
    "        done = False\n",
    "        s = env.reset(training=True)\n",
    "\n",
    "        # internally the episode length is fixed by trading_period\n",
    "        while not done:\n",
    "            logits = pi(s)\n",
    "            a = sample_action(logits, random=np.random.rand() <= rand_action_prob)\n",
    "            r, next_s, done = env.step(a.numpy())\n",
    "\n",
    "            # save the episode\n",
    "            all_logits.append(logits)\n",
    "            all_actions.append(a)\n",
    "            all_rewards.append(r)\n",
    "            \n",
    "            average_total_r += r\n",
    "            \n",
    "            # debugging\n",
    "#             print(env.t)\n",
    "#             print(env.t+1==200)\n",
    "#             print(r[0])\n",
    "#             print(env.total_portfolio_value[0])\n",
    "#             print(done)\n",
    "\n",
    "        all_logits_stack = tf.stack(all_logits)\n",
    "        all_actions_stack = tf.stack(all_actions)\n",
    "        all_rewards_stack = np.array(all_rewards)\n",
    "        \n",
    "        # compute cummulative rewards for each action\n",
    "        all_cum_rewards = discount_rewards(all_rewards_stack, all_actions_stack.numpy())\n",
    "        all_cum_rewards -= np.mean(all_cum_rewards)\n",
    "        all_cum_rewards /= np.std(all_cum_rewards)\n",
    "        all_cum_rewards = tf.convert_to_tensor(all_cum_rewards, dtype=tf.float32)\n",
    "\n",
    "        loss_value = loss(all_logits_stack, all_actions_stack, all_cum_rewards)\n",
    "    \n",
    "    if (batch+1) % batch_per_print == 0:\n",
    "        print(\"batch id: {}, average_total_r_per_ep: {}\".format(batch, np.mean(average_total_r/batch_per_print)))\n",
    "        average_total_r = np.zeros(batch_size)\n",
    "    \n",
    "    grads = gt.gradient(loss_value, state_encoding_model.variables + pi.variables)\n",
    "    optimizer.apply_gradients(zip(grads, state_encoding_model.variables + pi.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.58472873919163\n",
      "10065.584728739192\n",
      "False\n",
      "231.73679497384728\n",
      "10297.321523713039\n",
      "False\n",
      "-11.104921338475833\n",
      "10286.216602374563\n",
      "False\n",
      "-132.14359293915368\n",
      "10154.07300943541\n",
      "False\n",
      "-19.123817203524595\n",
      "10134.949192231885\n",
      "False\n",
      "-511.3982681888883\n",
      "9623.550924042996\n",
      "False\n",
      "-96.72462945416919\n",
      "9526.826294588827\n",
      "False\n",
      "166.3945969855322\n",
      "9693.22089157436\n",
      "False\n",
      "-990.1806086711549\n",
      "8703.040282903205\n",
      "False\n",
      "35.5760066044586\n",
      "8738.616289507663\n",
      "False\n",
      "210.21152209094907\n",
      "8948.827811598612\n",
      "False\n",
      "-166.60123853521873\n",
      "8782.226573063394\n",
      "False\n",
      "-31.921103855082038\n",
      "8750.305469208311\n",
      "False\n",
      "-92.15600101744894\n",
      "8658.149468190863\n",
      "False\n",
      "146.78114853725128\n",
      "8804.930616728114\n",
      "False\n",
      "64.80608911345553\n",
      "8869.73670584157\n",
      "False\n",
      "-424.67758637966654\n",
      "8445.059119461903\n",
      "False\n",
      "-84.50903765040675\n",
      "8360.550081811496\n",
      "False\n",
      "167.19062392612796\n",
      "8527.740705737624\n",
      "False\n",
      "90.21140818174354\n",
      "8617.952113919368\n",
      "False\n",
      "75.36665374822587\n",
      "8693.318767667593\n",
      "False\n",
      "-2.421406508605287\n",
      "8690.897361158988\n",
      "False\n",
      "220.12284884711698\n",
      "8911.020210006105\n",
      "False\n",
      "-1412.3090586522785\n",
      "7498.711151353827\n",
      "False\n",
      "-419.56003149622757\n",
      "7079.151119857599\n",
      "False\n",
      "379.2501055067278\n",
      "7458.401225364327\n",
      "False\n",
      "-318.1239970006209\n",
      "7140.277228363706\n",
      "False\n",
      "223.8729359996396\n",
      "7364.1501643633455\n",
      "False\n",
      "-457.8521373047497\n",
      "6906.298027058596\n",
      "False\n",
      "128.43340744509078\n",
      "7034.731434503687\n",
      "False\n",
      "-296.6032938607277\n",
      "6738.128140642959\n",
      "False\n",
      "-342.53522977891225\n",
      "6395.592910864047\n",
      "False\n",
      "283.446334997283\n",
      "6679.03924586133\n",
      "False\n",
      "84.96846400845061\n",
      "6764.00770986978\n",
      "False\n",
      "59.868091693097085\n",
      "6823.875801562877\n",
      "False\n",
      "-74.1382845361968\n",
      "6749.7375170266805\n",
      "False\n",
      "-214.45581023123486\n",
      "6535.281706795446\n",
      "False\n",
      "-177.6263565567715\n",
      "6357.655350238674\n",
      "False\n",
      "-83.76517265439361\n",
      "6273.8901775842805\n",
      "False\n",
      "-559.5920039818284\n",
      "5714.298173602452\n",
      "False\n",
      "16.396906656251304\n",
      "5730.6950802587035\n",
      "False\n",
      "-82.22215991767735\n",
      "5648.472920341026\n",
      "False\n",
      "-23.133265136050795\n",
      "5625.339655204975\n",
      "False\n",
      "-13.241889209110013\n",
      "5612.097765995865\n",
      "False\n",
      "27.647168113818225\n",
      "5639.7449341096835\n",
      "False\n",
      "201.49891694582766\n",
      "5841.243851055511\n",
      "False\n",
      "-81.31356125907314\n",
      "5759.930289796438\n",
      "False\n",
      "-373.8424276708456\n",
      "5386.087862125592\n",
      "False\n",
      "-258.62984403416976\n",
      "5127.458018091423\n",
      "False\n",
      "200.11551084296116\n",
      "5327.573528934384\n",
      "False\n",
      "-220.34799228313113\n",
      "5107.225536651253\n",
      "False\n",
      "-173.7069659986546\n",
      "4933.518570652598\n",
      "False\n",
      "15.543590742332526\n",
      "4949.062161394931\n",
      "False\n",
      "114.64771881800516\n",
      "5063.709880212936\n",
      "False\n",
      "140.67719790692627\n",
      "5204.387078119862\n",
      "False\n",
      "-69.28978720451141\n",
      "5135.097290915351\n",
      "False\n",
      "334.7956455230951\n",
      "5469.892936438446\n",
      "False\n",
      "62.73922778799715\n",
      "5532.632164226443\n",
      "False\n",
      "-13.137008034215796\n",
      "5519.495156192227\n",
      "False\n",
      "-73.90857135717579\n",
      "5445.586584835051\n",
      "False\n",
      "78.35749481851053\n",
      "5523.944079653562\n",
      "False\n",
      "-35.71621969480748\n",
      "5488.227859958754\n",
      "False\n",
      "-261.47133252809\n",
      "5226.756527430664\n",
      "False\n",
      "1546.9915339324907\n",
      "6773.748061363155\n",
      "False\n",
      "444.3896744026342\n",
      "7218.137735765789\n",
      "False\n",
      "-79.81045018082114\n",
      "7138.327285584968\n",
      "False\n",
      "268.32683869754055\n",
      "7406.654124282509\n",
      "False\n",
      "-299.80902430954393\n",
      "7106.845099972965\n",
      "False\n",
      "-21.83021963583724\n",
      "7085.014880337128\n",
      "False\n",
      "-35.96075667432069\n",
      "7049.054123662807\n",
      "False\n",
      "-144.53673597735178\n",
      "6904.517387685455\n",
      "False\n",
      "-97.90083672161654\n",
      "6806.616550963839\n",
      "False\n",
      "259.1496801656376\n",
      "7065.766231129476\n",
      "False\n",
      "119.20665048297724\n",
      "7184.972881612453\n",
      "False\n",
      "123.65557394431198\n",
      "7308.628455556765\n",
      "False\n",
      "131.75427312628563\n",
      "7440.382728683051\n",
      "False\n",
      "306.62462882053387\n",
      "7747.007357503585\n",
      "False\n",
      "-10.390703943442531\n",
      "7736.616653560142\n",
      "False\n",
      "-91.10005601111698\n",
      "7645.516597549025\n",
      "False\n",
      "-94.54062666769642\n",
      "7550.975970881329\n",
      "False\n",
      "61.1910880225405\n",
      "7612.1670589038695\n",
      "False\n",
      "56.07810288211385\n",
      "7668.245161785983\n",
      "False\n",
      "-13.636336050720274\n",
      "7654.608825735263\n",
      "False\n",
      "272.37161854551687\n",
      "7926.98044428078\n",
      "False\n",
      "199.0672563797416\n",
      "8126.0477006605215\n",
      "False\n",
      "54.31050399511878\n",
      "8180.35820465564\n",
      "False\n",
      "530.278105698635\n",
      "8710.636310354275\n",
      "False\n",
      "23.173166794509598\n",
      "8733.809477148785\n",
      "False\n",
      "-72.76256897643907\n",
      "8661.046908172346\n",
      "False\n",
      "57.98792108809539\n",
      "8719.034829260441\n",
      "False\n",
      "28.52099214270129\n",
      "8747.555821403143\n",
      "False\n",
      "-55.004770560923134\n",
      "8692.55105084222\n",
      "False\n",
      "37.57331867047651\n",
      "8730.124369512696\n",
      "False\n",
      "-134.56555673348885\n",
      "8595.558812779207\n",
      "False\n",
      "-112.86529613052153\n",
      "8482.693516648686\n",
      "False\n",
      "111.57707443079744\n",
      "8594.270591079483\n",
      "False\n",
      "-1.947713635805485\n",
      "8592.322877443677\n",
      "False\n",
      "-181.22653384719706\n",
      "8411.09634359648\n",
      "False\n",
      "-75.80612927703987\n",
      "8335.29021431944\n",
      "False\n",
      "21.076100582340587\n",
      "8356.366314901781\n",
      "False\n",
      "-167.9943414098343\n",
      "8188.371973491947\n",
      "False\n",
      "162.3723314811541\n",
      "8350.744304973101\n",
      "False\n",
      "84.19952115446358\n",
      "8434.943826127565\n",
      "False\n",
      "-81.91777045047274\n",
      "8353.026055677092\n",
      "False\n",
      "-143.12880930224492\n",
      "8209.897246374847\n",
      "False\n",
      "-47.140354300981016\n",
      "8162.756892073866\n",
      "False\n",
      "153.25959517995307\n",
      "8316.016487253819\n",
      "False\n",
      "-383.2390453242879\n",
      "7932.777441929531\n",
      "False\n",
      "105.06073235827171\n",
      "8037.838174287803\n",
      "False\n",
      "118.0233099581501\n",
      "8155.861484245953\n",
      "False\n",
      "-57.24661960644971\n",
      "8098.614864639503\n",
      "False\n",
      "-2.2019473870659567\n",
      "8096.412917252437\n",
      "False\n",
      "413.6627224155909\n",
      "8510.075639668028\n",
      "False\n",
      "225.97512924055445\n",
      "8736.050768908583\n",
      "False\n",
      "8.009199093296047\n",
      "8744.059968001879\n",
      "False\n",
      "-308.21723998745983\n",
      "8435.842728014419\n",
      "False\n",
      "-183.48320669321583\n",
      "8252.359521321203\n",
      "False\n",
      "242.6370809912769\n",
      "8494.99660231248\n",
      "False\n",
      "30.3934722045924\n",
      "8525.390074517072\n",
      "False\n",
      "212.58845719808414\n",
      "8737.978531715156\n",
      "False\n",
      "-55.32911085735759\n",
      "8682.649420857799\n",
      "False\n",
      "-149.87430726805906\n",
      "8532.77511358974\n",
      "False\n",
      "14.709668371910084\n",
      "8547.48478196165\n",
      "False\n",
      "57.8058001106474\n",
      "8605.290582072297\n",
      "False\n",
      "283.2964251351859\n",
      "8888.587007207483\n",
      "False\n",
      "210.4862639571711\n",
      "9099.073271164654\n",
      "False\n",
      "-328.40926248343567\n",
      "8770.664008681219\n",
      "False\n",
      "805.0198604653488\n",
      "9575.683869146567\n",
      "False\n",
      "168.46914885410115\n",
      "9744.153018000668\n",
      "False\n",
      "83.38554961327463\n",
      "9827.538567613943\n",
      "False\n",
      "320.4701699353118\n",
      "10148.008737549255\n",
      "False\n",
      "30.61805835487212\n",
      "10178.626795904127\n",
      "False\n",
      "67.79180315499798\n",
      "10246.418599059125\n",
      "False\n",
      "238.64758388798327\n",
      "10485.066182947108\n",
      "False\n",
      "-42.242258539083196\n",
      "10442.823924408025\n",
      "False\n",
      "-263.9873661541096\n",
      "10178.836558253915\n",
      "False\n",
      "-75.24182174409907\n",
      "10103.594736509816\n",
      "False\n",
      "48.07403081755365\n",
      "10151.66876732737\n",
      "False\n",
      "190.36379072063937\n",
      "10342.03255804801\n",
      "False\n",
      "-45.22284555188526\n",
      "10296.809712496124\n",
      "False\n",
      "-66.38900350863332\n",
      "10230.42070898749\n",
      "False\n",
      "460.7631750581113\n",
      "10691.183884045602\n",
      "False\n",
      "59.063816923657214\n",
      "10750.24770096926\n",
      "False\n",
      "345.7056283122729\n",
      "11095.953329281532\n",
      "False\n",
      "103.71778519959844\n",
      "11199.67111448113\n",
      "False\n",
      "-84.47939004942964\n",
      "11115.191724431701\n",
      "False\n",
      "253.03291196346072\n",
      "11368.224636395162\n",
      "False\n",
      "-95.7935164519622\n",
      "11272.4311199432\n",
      "False\n",
      "-173.54167505033547\n",
      "11098.889444892864\n",
      "False\n",
      "13.681364737922195\n",
      "11112.570809630786\n",
      "False\n",
      "-1392.2612617039267\n",
      "9720.30954792686\n",
      "False\n",
      "8.263432844558338\n",
      "9728.572980771418\n",
      "False\n",
      "-212.43854733598528\n",
      "9516.134433435433\n",
      "False\n",
      "-97.9356113513386\n",
      "9418.198822084094\n",
      "False\n",
      "-26.64851208081018\n",
      "9391.550310003284\n",
      "False\n",
      "227.29812556999605\n",
      "9618.84843557328\n",
      "False\n",
      "51.83894177056936\n",
      "9670.68737734385\n",
      "False\n",
      "10.066363647007165\n",
      "9680.753740990856\n",
      "False\n",
      "87.42007540376835\n",
      "9768.173816394625\n",
      "False\n",
      "76.75462959410288\n",
      "9844.928445988728\n",
      "False\n",
      "-126.10205831089115\n",
      "9718.826387677836\n",
      "False\n",
      "-91.96875301125874\n",
      "9626.857634666578\n",
      "False\n",
      "-55.0646230486127\n",
      "9571.793011617965\n",
      "False\n",
      "-181.3262879933518\n",
      "9390.466723624613\n",
      "False\n",
      "90.43086730328469\n",
      "9480.897590927898\n",
      "False\n",
      "-100.46245632057253\n",
      "9380.435134607325\n",
      "False\n",
      "40.7391474608321\n",
      "9421.174282068157\n",
      "False\n",
      "147.6575360805018\n",
      "9568.83181814866\n",
      "False\n",
      "278.5927106368563\n",
      "9847.424528785516\n",
      "False\n",
      "-41.73780349383924\n",
      "9805.686725291676\n",
      "False\n",
      "-178.97955777292918\n",
      "9626.707167518747\n",
      "False\n",
      "-103.80728528826876\n",
      "9522.899882230478\n",
      "False\n",
      "424.8694041288618\n",
      "9947.76928635934\n",
      "False\n",
      "-282.4548122216529\n",
      "9665.314474137687\n",
      "False\n",
      "-304.02767730612686\n",
      "9361.28679683156\n",
      "False\n",
      "247.15354748341815\n",
      "9608.440344314979\n",
      "False\n",
      "-229.88170222681947\n",
      "9378.55864208816\n",
      "False\n",
      "-29.63422612235263\n",
      "9348.924415965806\n",
      "False\n",
      "-64.11694957639156\n",
      "9284.807466389415\n",
      "False\n",
      "385.8154887237324\n",
      "9670.622955113147\n",
      "False\n",
      "16.38264014149354\n",
      "9687.00559525464\n",
      "False\n",
      "-79.89538052648459\n",
      "9607.110214728156\n",
      "False\n",
      "-8.268559873300546\n",
      "9598.841654854856\n",
      "False\n",
      "-258.4502865710929\n",
      "9340.391368283763\n",
      "False\n",
      "54.330454824350454\n",
      "9394.721823108113\n",
      "False\n",
      "-58.09536577736071\n",
      "9336.626457330753\n",
      "False\n",
      "326.5618602795148\n",
      "9663.188317610267\n",
      "False\n",
      "57.366324581837034\n",
      "9720.554642192104\n",
      "False\n",
      "-184.43683403902287\n",
      "9536.117808153082\n",
      "False\n",
      "-53.28677010413594\n",
      "9482.831038048946\n",
      "False\n",
      "-80.4346102014515\n",
      "9402.396427847494\n",
      "False\n",
      "98.06044335546358\n",
      "9500.456871202958\n",
      "False\n",
      "347.42819851206696\n",
      "9847.885069715025\n",
      "False\n",
      "69.37471755017577\n",
      "9917.2597872652\n",
      "False\n",
      "-131.20991642257832\n",
      "9786.049870842622\n",
      "False\n",
      "-12.243233176102876\n",
      "9773.80663766652\n",
      "False\n",
      "-31.092308513401804\n",
      "9742.714329153117\n",
      "False\n",
      "207.43044337045285\n",
      "9950.14477252357\n",
      "False\n",
      "-35.25679333676635\n",
      "9914.887979186804\n",
      "True\n",
      "At test time, average_total_r_per_ep: 26106.825338423874\n"
     ]
    }
   ],
   "source": [
    "# test time\n",
    "average_total_r = np.zeros(batch_size)\n",
    "done = False\n",
    "s = env.reset()\n",
    "\n",
    "# internally the episode length is fixed by trading_period\n",
    "while not done:\n",
    "    logits = pi(s)\n",
    "    a = sample_action(logits)\n",
    "    r, next_s, done = env.step(a.numpy())\n",
    "    \n",
    "#     print(r[0])\n",
    "#     print(env.total_portfolio_value[0])\n",
    "#     print(done)\n",
    "\n",
    "    average_total_r += r\n",
    "\n",
    "print(\"At test time, average_total_r_per_ep: {}\".format(np.mean(average_total_r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2018 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2018u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
